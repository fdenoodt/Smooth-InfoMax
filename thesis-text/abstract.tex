\chapter*{ Abstract}
This thesis introduces Variational Greedy InfoMax (V-GIM), a self-supervised representation learning method that incorporates an interpretability constraint into both the network and the loss function. V-GIM's architecture is split up into modules, each individually optimised to promote interpretability by imposing constraints on their respective latent spaces. This approach enables the analysis of the underlying structures in the internal and final representations.

Inspired by Variational Autoencoders (VAE), V-GIM represents each module's representations as samples from Gaussian distributions. These representations are optimised to maximise the mutual information between temporally nearby patches using the InfoNCE bound introduced in \citep{oordRepresentationLearningContrastive2019}. However, V-GIM introduces a regularisation term to the loss function, encouraging distributions to approximate the standard normal distribution, thereby constraining the latent space of each module.


By enforcing these latent space constraints, V-GIM ensures that sampling a random point from the standard normal distribution within a specific latent space is likely to correspond to a meaningful data point in the original space, similar to the points in the dataset. This constraint also promotes smooth transitions in the latent space with respect to the mutual information. V-GIM's latent space is utilised to train a decoder, enabling the analysis of the underlying structure of representations at different levels in the architecture while ensuring good generalisation.

Additionally, we argue that V-GIM's latent space constraint is related to theories from VAEs leading to disentangled representations, which could potentially enable easier analysis of the model through post-hoc explainability approaches. 

We evaluate V-GIM's performance on sequential speech data. V-GIM achieves similar performance to its less interpretable counterpart, GIM, and even outperforms it in deeper modules due to V-GIM's internal batch normalisation mechanism. We examine the potential of V-GIM's representation variance as a data augmentation tool for downstream tasks with limited labelled data and demonstrate that the variability in representations does not contribute to improved performance.

Finally, we provide insights into V-GIM's internal representations. We demonstrate that the representations learn to differentiate between vowel information while consonant information appears to be less prominent. Additionally, we discuss which internal neurons are sensitive to which vowels, further demonstrating the interpretability of V-GIM.

Our code is available via \href{https://github.com/oBoii/Variational-Greedy-InfoMax}{GitHub}.
 




%Consequently, when interpolating between two latent representations within a given module, the resulting point is more likely to correspond to dataset points in the original space. 

 


% This approach results in interpretable representations at different layers in the network architecture, allowing for easier analysis of the internal workings of the neural network.

%We demonstrate that V-GIM's latent space constraints enables easier analysis of the model through post-hoc explainability approaches. 
%The latent space constraints ensure that the dataset covers the entire region around in origin in the latent space. As such, interpolating between two latent representations correspond to a meaningful data point in the original space, similar to the dataset. 


%Through the latent space constraints, the dataset is more encouraged to take in the entire latent space around the origin, ensuring that 
%randomly sampling a data point in 
%A decoder is trained

%This results in a predictable latent space, in which the dataset is likely to take in the entire latent space around the origin. Post-Hoc



%V-GIM is split up into modules, which are each greedily trained with a novel contrastive loss function, maximising the mutual information while imposing constraints to their latent space. The latent space from each module is optimised to be standard normal, 


% *** FOR INTRODUCTION OR ABSTRACT? ***
% V-GIM's latent space constraints ensure that a decoder trained on this space would generalise well to unseen data and predict new data points similar to the dataset, improving interpretability of reconstructions, allowing for meaningful interpolations between representations.

% vgims latent space constraint ensures that sampling a random point from a standard normal distribution is likely to correspond to a meaningful data point in the original space. As such, decoding the interpolation of two latent representations is more likely to result into a meaningful reconstruction, improving interpretability obtained form the decoder. Enabling us to analyse the underlying structure of internal and final representations.


% (and ensure that the data corresponding to high neuron activations correspond to data points that are similar to the dataset)