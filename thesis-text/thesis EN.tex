\documentclass[]{book}
\usepackage{vub}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{subcaption}

%TODO: OVERAL EMBEDDED SPACE GEBRUIKEN.



\usepackage[toc,page]{appendix}

\usepackage{listofitems} % for \readlist to create arrays
\usetikzlibrary{arrows.meta} % for arrow size
\usepackage[outline]{contour} % glow around text
\contourlength{1.4pt}
\tikzstyle{mynode}=[thick,draw=blue,fill=blue!20,circle,minimum size=22]

\tikzset{>=latex} % for LaTeX arrow head
\usepackage{xcolor}
\colorlet{myred}{red!80!black}
\colorlet{myblue}{blue!80!black}
\colorlet{mygreen}{green!60!black}
\colorlet{myorange}{orange!70!red!60!black}
\colorlet{mydarkred}{red!30!black}
\colorlet{mydarkblue}{blue!40!black}
\colorlet{mydarkgreen}{green!30!black}
\tikzstyle{node}=[thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]
\tikzstyle{node in}=[node,green!20!black,draw=mygreen!30!black,fill=mygreen!25]
\tikzstyle{node hidden}=[node,blue!20!black,draw=myblue!30!black,fill=myblue!20]
\tikzstyle{node convol}=[node,orange!20!black,draw=myorange!30!black,fill=myorange!20]
\tikzstyle{node out}=[node,red!20!black,draw=myred!30!black,fill=myred!20]
\tikzstyle{connect}=[thick,mydarkblue] %,line cap=round
\tikzstyle{connect arrow}=[-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1]
\tikzset{ % node styles, numbered for easy mapping with \nstyle
	node 1/.style={node in},
	node 2/.style={node hidden},
	node 3/.style={node out},
}
\def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3


\usetikzlibrary{shapes.geometric, arrows}
\usepackage[pdftex]{pict2e}




%\includeonly{background}
%\includeonly{variationalcontrastivepredictivecoding}
%\includeonly{experiments}





\pagenumbering{gobble}


\title{Variational Greedy InfoMax}
\subtitle{Towards independent and interpretable representations}
\author{Fabian Denoodt}
\faculty{Science and Bio-Engineering Sciences}
\promotors{Promotor(s):~Prof. Dr. Bart de Boer}
\pretitle{Master thesis submitted in partial fulfilment of the requirements for the degree of Master of Science In de Ingenieurswetenschappen: Computerwetenschappen}
\date{2022-2023}


\begin{document}
\maketitle
\title{[Dutch] Variational Greedy InfoMax}
\subtitle{[Dutch] Towards independent and interpretable representations}
\pretitle{Proefschrift ingediend met het oog op het behalen van de graad van Master of Science In de Ingenieurswetenschappen: Computerwetenschappen}
\faculty{Wetenschappen en Bio-ingenieurswetenschappen}
\maketitle

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Overlay Generator - Annotated Figures v0.0.1
% Created with http://ff.cx/latex-overlay-generator/
% If this generator saves you time, consider donating 5,- EUR! :-)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\annotatedFigureBoxCustom{bottom-left}{top-right}{label}{label-position}{box-color}{label-color}{border-color}{text-color}
\newcommand*\annotatedFigureBoxCustom[8]{\draw[#5,thick,rounded corners] (#1) rectangle (#2);\node at (#4) [fill=#6,thick,shape=circle,draw=#7,inner sep=2pt,font=\sffamily,text=#8] {\textbf{#3}};}

%\annotatedFigureBox{bottom-left}{top-right}{label}{label-position}
\newcommand*\annotatedFigureBox[4]{\annotatedFigureBoxCustom{#1}{#2}{#3}{#4}{white}{white}{black}{black}}

%\newcommand*\annotatedFigureText[4]{\node[draw=none, anchor=south west, text=#2, inner sep=0, text width=#3\linewidth,font=\sffamily] at (#1){#4};}

\newcommand*\annotatedFigureText[5]{\node[draw=none, anchor=south west, text=#2, inner sep=0, text width=#3\linewidth,font=\sffamily\fontsize{#5}{14}\selectfont] at (#1){#4};}


\newenvironment {annotatedFigure}[1]{\centering\begin{tikzpicture}
		\node[anchor=south west,inner sep=0] (image) at (0,0) { #1};\begin{scope}[x={(image.south east)},y={(image.north west)}]}{\end{scope}\end{tikzpicture}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vecti}[1]{\mathbf{#1}^{(i)}}
\newcommand{\vectj}[1]{\mathbf{#1}^{(j)}}

\newcommand{\xith}[0]{\vecti{x}}
\newcommand{\zith}[0]{\vecti{z}}

\newcommand{\kl}[2]{D_{KL} \left( #1 \mid \mid #2 \right)}
\newcommand{\condp}[2]{p(#1 \mid #2)} % conditional probability
\newcommand{\condq}[2]{q(#1 \mid #2)} % conditional probability
\newcommand{\probzx}[0]{\condp{\vect{z}}{\vecti{x}}} % conditional probability
\newcommand{\probxz}[0]{\condp{\vecti{x}}{\vect{z}}} % conditional probability
\newcommand{\qprobzx}[0]{\condq{\vect{z}}{\vecti{x}}} % conditional probability
\newcommand{\qprobqxz}[0]{\condq{\vecti{x}}{\vect{z}}} % conditional probability

%_{x \in p_{data}(x),\\ z \in p_{z}(z)}}
\newcommand{\expected}[1]{\mathop{\mathbb{E}}_{\substack{#1}}}

\newcommand{\naturalset}[0]{\mathbb{N}}

\newcommand{\sample}[2]{#1 \sim #2}
\newcommand{\expectedsample}[2]{\expected{\sample{#1}{#2}}}
\newcommand{\expectedsamplezq}[0]{\expectedsample{\vecti{z}}{q (\cdot \mid \vecti{x})}}
%\newcommand{\expectedsamplezq}[0]{\expectedsample{\vect{z}}{q_\phi (\cdot \mid \vecti{x})}}

\newcommand{\pzx}[0]{\probzx}
\newcommand{\pxz}[0]{\probxz}

\newcommand{\qzx}[0]{\qprobzx}
\newcommand{\qxz}[0]{\qprobxz}




% p_theta(z|x)





\newcommand{\qz}[0]{p(\vect{z})}
\newcommand{\pz}[0]{p_\theta(\vect{Z})}
%\newcommand{\pzblank}[0]{p_\theta(\cdot)}
\newcommand{\pzblank}[0]{p(\cdot)}


\newcommand{\px}[0]{p( \vecti{x} )}

\newcommand{\elbo}[0]{  \expectedsamplezq \left[ \log p(\vect{Z}, \vecti{x}) - \log \qzx \right]  }
\newcommand{\logevidence}[0]{\log \px}

\newcommand{\lelbo}[0]{\mathcal{L}_{\textit{ELBO}}}

\newcommand{\klapproxposterior}[0]{\kl{\qprobzx}{\probzx}}


\newcommand{\reconstr}[0]{\expectedsamplezq \left[ -\log \pthetaxzi \right]}



\newcommand{\latentspaceconstraint}[0]{\kl{ \qphizxblank }{ \pzblank } }

\newcommand{\latentspaceconstraintstandardgaussian}[0]{\kl{ \qphizxblank }{ \standardnormal } }

\newcommand{\latentspaceconstraintclosedform}[0]{ \frac{1}{2} \sum_{k=1}^{D} \left( -\log (\sigma_k^{(i)})^2 - 1 + (\sigma_k^{(i)})^2 +  (\mu_k^{(i)})^2 \right) }


\newcommand{\latentspaceconstraintclosedformNoI}[0]{ \frac{1}{2} \sum_{k=1}^{D} \left( -\log \sigma_k^2 - 1 + \sigma_k^2 +  \mu_k^2 \right) }


\newcommand{\elboexplicit}[0]{ \reconstr + \latentspaceconstraint }
\newcommand{\betaelboexplicit}[0]{ \reconstr + \beta~ \latentspaceconstraint }

\newcommand{\mufat}[0]{\bm{\mu}}
\newcommand{\sigmafat}[0]{\bm{\sigma}}

\newcommand{\mui}[0]{\bm{\mu}^{(i)}}
\newcommand{\sigmai}[0]{\bm{\sigma}^{(i)}}
\newcommand{\epiloni}[0]{\bm{\epsilon}^{(i)}}

\newcommand{\epilonfat}[0]{\bm{\epsilon}}

\newcommand{\identitymtx}[0]{\mathbf{I}}
\newcommand{\covariancemtx}[0]{\mathbf{\Sigma}}

%\newcommand{\sigmaisq}[0]{(\bm{\sigma}^{(i)})^2 }
\newcommand{\diagsigmai}[0]{\text{diag}(\sigmai)} % maybe should be sigmaisq
\newcommand{\diagsigma}[0]{\text{diag}(\sigmafat)} % maybe should be sigmaisq
\newcommand{\normal}[0]{\mathcal{N}(\mui, \diagsigmai)}
\newcommand{\normalNoI}[0]{\mathcal{N}(\mufat , \diagsigma)}
\newcommand{\standardnormal}[0]{\mathcal{N}(\bm{\vect{0}}, \identitymtx)}

\newcommand{\qphizx}[0]{q (\vect{z} \mid \vecti{x})}
%\newcommand{\qphizx}[0]{q_\phi (\vect{z} \mid \vecti{x})}

\newcommand{\qvae}[0]{q (\vect{z} \mid \vect{x})}
\newcommand{\qvaei}[0]{q (\vecti{z} \mid \vecti{x})}
\newcommand{\qvaeEmptyZandXI}[0]{q ( \cdot \mid \vecti{x})}

\newcommand{\qphizxblank}[0]{q (\cdot \mid \vecti{x})}
%\newcommand{\qphizxblank}[0]{q_\phi (\cdot \mid \vecti{x})}

%\newcommand{\pthetaxz}[0]{p_\theta (\vecti{x} \mid \vect{z})}
\newcommand{\pthetaxz}[0]{p (\vecti{x} \mid \vect{z})} % TODO: I REMOVED THETA BECAUSE I DON'T UNDERSTAND IT.
\newcommand{\pthetaxzi}[0]{p (\vecti{x} \mid \vecti{z})}


\newcommand{\tildexi}[0]{\widetilde{\mathbf{x}} ^ {(i)}}
\newcommand{\tildex}[0]{\widetilde{\mathbf{x}}}

\newcommand{\samplestandardnormal}[1]{\sample{#1}{\standardnormal}}
	

\newcommand{\W}{\mathbf{W}}	

%cpc

\newcommand{\fzc}[0]{f_k(\ztk, \ct)}
\newcommand{\pztk}[0]{p(\ztk)}
\newcommand{\pzcdivpz}[0]{\frac{p(\ztk \mid \ct)}{\pztk}}

\newcommand{\mutinfztkct}[0]{I(z_{t+k}, c_t)}


\newcommand{\x}[0]{\vect{x}}
\newcommand{\xt}[0]{\x_t}
\newcommand{\xtk}[0]{\vect{x}_{t+k}}
\newcommand{\xtone}[0]{\vect{x}_{t+1}}

\newcommand{\z}[0]{\vect{z}}
\newcommand{\zt}[0]{\z_t}
\newcommand{\zi}[0]{\z_i}
\newcommand{\zj}[0]{\z_j}
\newcommand{\ztk}[0]{\vect{z}_{t+k}}
\newcommand{\ztone}[0]{\vect{z}_{t+1}}

\newcommand{\ct}[0]{\vect{c}_t}
\newcommand{\ctk}[0]{\vect{c}_{t+k}}
\newcommand{\ctone}[0]{\vect{c}_{t+1}}

\newcommand{\fkdefinition}[0]{f_k(\z_j, \ct) = \exp(\z_j^T W_k \ct)}

\newcommand{\fkzc}[0]{f_k(\z_{t+k}, \ct)}

\newcommand{\nceprediction}[0]{\frac{\fkzc}{\sum_{\zj \in X} f_k(\zj, \ct)}}



% GIM

\newcommand{\fkblank}[0]{f_k(\cdot)}
\newcommand{\fkmblank}[0]{f_k^m(\cdot)}

\newcommand{\fkm}[0]{f_k^m(\ztk^m,\zt^m)}


\newcommand{\gim}[0]{\log \frac{\fkm }{\sum_{\zj^m \in X} f_k^m(\zj^m, \zt^m)}}


\newcommand{\R}[0]{\mathbb{R}}

\newcommand{\qzzblank}[0]{q(\cdot \mid \zt^{m-1})}

\newcommand{\latentspaceconstraintgim}[0]{\kl{ \qzzblank }{ \standardnormal } }

\newcommand{\sampleqdot}[1]{q(\cdot \mid #1)}


\newcommand{\reconstrgim}[0]{\sum_k
	\expected{
		\sample{\ztk^m}{\sampleqdot{\ztk^{m-1}} } \\ 
		\sample{\zt^m}{\sampleqdot{\zt^{m-1}}} } 
	\left[ \gim \right]
}


\newcommand{\genc}[0]{g_{enc}(\cdot)}
\newcommand{\gencm}[0]{g_{enc}^m(\cdot)}
\newcommand{\gencM}[0]{g_{enc}^M(\cdot)}

\newcommand{\gar}[0]{g_{ar}(\cdot)}

\newcommand{\Lvnce}[0]{\mathcal{L}_{\text{V-NCE}}}
\newcommand{\Lnce}[0]{\mathcal{L}_{\text{NCE}}}

\newcommand{\qfromzmneg}[0]{\sampleqdot{\zt^{m-1}}}
\newcommand{\qfromzm}[0]{\sampleqdot{\zt^m}}
\newcommand{\qfromzM}[0]{\sampleqdot{\zt^M}}

\newcommand{\normalfatmusigma}[0]{\mathcal{N}(\mufat, \text{diag}(\sigmafat^2))}



\newcommand{\D}[0]{\mathcal{D}}
\newcommand{\X}[0]{\mathcal{X}}
\newcommand{\Z}[0]{\mathcal{Z}}
\newcommand{\Y}[0]{\mathcal{Y}}
\newcommand{\f}[0]{f(\cdot) }
\newcommand{\T}[0]{T(\cdot) }
\newcommand{\E}[0]{E(\cdot) }

\newcommand{\Dtrain}[0]{\D_{\text{train}}}

\newcommand{\xtith}[0]{\xt^{(i)}}
\newcommand{\xtjth}[0]{\xt^{(j)}}
\newcommand{\ztm}[0]{\zt^{m}}
\newcommand{\ztmnegone}[0]{\zt^{m-1}}

\include{introduction}
\include{background}
\include{variationalcontrastivepredictivecoding}
\include{experiments}
\include{relatedwork}

\chapter{Discussion}

---
batch norm:
 - sindy didn't have issues of batch norm, but believe this is because each module consisted of a single layer, ours contain a number of layers. potentially: outputs from first module change too fast for second module to catch up.




while GIM argues to resolve memory constraints, not entirely true. In fact we even countered the opposite as containing multiple neural networks, each with their own personal loss function (the loss function is based on fk which contains parameters that must be learned), and thus for early layers where the sequence is still long, a lot of memory is required. We went for a compromise on GIM by splitting up the architecture in merely two modules, significantly reducing the memory constraints.


---
The second module in GIM clearly doesn't have as much effect. This can be explained because there may not be as much common information anymore between the patches. There may be a source that says that cpc learns low level features, but the second module is supposed to learn more high level features, which cpc may have trouble with?
---

Future work:
 - Related work in VAE shows that gradually increasing regularisation term, results in better disentengledment, while avoiding posterior collapse. could have a kldweight scheduler.
 
- not constrained solely to InfoNCE loss, the GIM architecture could work for other losses too that allow for greedy optimisation.


- I didn't add an autoregressor as i didn't find a performance benefit. Potentially, with larger architecture could further improve performance.



----
Towards production setting:
	encodings are thus optimised to be close the standard normal. When in a production environment and new data is given, could in fact have an idea of how well generalisation to the production data: eg via anomaly detection if encodings are too far away from center. 
	= gives automated way of verifying generalisation.
	
	can then maybe see to which data that doesn't generalise well via outliers.
	
----


future work:
- disentanglement should do more investations


---
GIM: Modular training
could incrementally increase numb of modules and observe performance increase for downstream tasks.
based on this, could find smallest gim architecture depth which satisfies required accuracies.

----
interpretability:
most dimensions sensitive around 75 to 150 hz. this is as expected as the adult man speaks around 80 to 180 hz.



\begin{itemize}
	\item Explainability of latents is dependent on the performance of the decoder.
	\item Intermediate loss function with kld resulted in similar behaviour as batch normalisation. Resulting in faster convergence than without kld.
	\item We observed no quality loss in the learned representations. Data was equally easily separable.
\end{itemize}



\bibliographystyle{IEEEtran}
\bibliography{biblio}


\include{appendix}

\end{document}
