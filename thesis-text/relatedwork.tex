%(van die YT VID) + book: https://christophm.github.io/interpretable-ml-book/neural-networks.html
%What about internals (look at neural representations):
%Our method is more like this approach, both explaining internals
%- feature visualisation
%focus on specific neuron, and use it to generate images to activate it.
%use the behaviour of that model and find image that activates the neuron.
%-> isolate neurons that detect textures etc.
%(distill.pub/2017/feature-visualisation, distill/2021/multimodal-neurons)
%
%
%
%%Additionally, researchers have explored methods to explain black box models.
%%- Relevance weighting, ... 

%^^ hun prentjes die ze genereren die de maximum activatie generenen, zijn geen echte prentjes, en zou wrs niet even goed interpreteerbaar zijn voor audio data, vermits her oor gevoeliger is voor disturbance en dus minder goed kan verstaan.
%*******************************************************************************
\chapter{Relation to existing work} \label{cha:5}

We have studied the representations obtained by maximising the InfoNCE loss function. We achieved this through the addition of a regularisation term to the loss function, resulting in a space that is easier to analyse and understand. In this chapter, we give an overview of the existing literature that is relevant to our research. We begin with a discussion of representation learning techniques related to mutual information maximisation and slowly digress into interpretable representations. Furthermore, we give an overview of the existing regularisation techniques used for both improved generalisation and better representations. Finally, we give an overview of alternative priors that have been introduced to the VAE framework in the past decade.


\section{Mutual information and interpretable representation learning}
	%\cite{lowePuttingEndEndtoEnd2020}
	%\citep{lowePuttingEndEndtoEnd2020}
	%\citeyear{lowePuttingEndEndtoEnd2020}

	V-GIM is based on the ideas of mutual information maximisation introduced in CPC and GIM \citep{lowePuttingEndEndtoEnd2020, oordRepresentationLearningContrastive2019}. This is achieved by maximising the mutual information between temporally nearby patches, assuming common information between nearby data \citep{lowePuttingEndEndtoEnd2020}. Similar approaches utilising mutual information maximization for representation learning have also been explored.
	
	Deep InfoMax incorporates an ANN encoder which maximises the mutual information between input and output. This is achieved by incorporating knowledge about locality in the representations, resulting in locally-consistent information across structural locations \citep{hjelmLearningDeepRepresentations2019}. Meanwhile, InfoGAN, C-DSVAE and S3VAE each introduce a different flavour on the mutual information maximisation scheme, while obtaining disentangled representations \citep{chenInfoGANInterpretableRepresentation2016, baiContrastivelyDisentangledSequential2021, zhuS3VAESelfSupervisedSequential2020}. 
	
	In addition, InfoGAN's learning approach results in interpretable representations. Manipulating an individual dimension in the latent space results in changes to a specific feature of the generated data while leaving other features unaffected. This approach is similar to how V-GIM achieves interpretability using an optional decoder. However, V-GIM takes this a step further by composing its architecture into one or more modules, each with its own interpretable representations. This enables the understanding of the final representations, but also the internal parts of the neural network, providing increased interpretability.
	
	Building upon InfoGAN's concepts, Bridge-GAN introduces an intermediate latent space, or ``bridge" between text and images. This approach enables the synthesis of interpretable and disentangled representations for text-to-image synthesis \citep{yuanBridgeGANInterpretableRepresentation2020}. Meanwhile, InfoVAEGAN's representations are learned by combining the framework of Generative Adversarial Networks with concepts from VAEs, enabling the learning of interpretable data variations \citep{yeInfoVAEGANLearningJoint2021}. Similar to V-GIM, latent representations are samples from a distribution, such that a single data point can have multiple latent representations.
	
	Continuing in the line of interpretable representation learning, Timeline uses recurrent neural networks with an attention mechanism to aggregate sequential health data to interpretable representations \citep{baiInterpretableRepresentationLearning2018}. Its interpretability is achieved through analysis of the weights associated with different medical codes. Similarly, \cite{agrawalInterpretableRepresentationLearning2020} apply relevance weighting on raw speech data, allowing for interpretation of the representations during forward propagation. 
	
	
	

\section{Regularisation}
	The practice of adding a penalty term to the loss function, known as regularization, has been widely used for various purposes. \cite{kukackaRegularizationDeepLearning2017} provide a survey categorising different regularisation techniques, including those that impose constraints on the weights, or on the activations.
	
	Regularisation terms that enforce constraints on the weights typically aim to improve generalisation performance by penalising complexity. Weight decay, for example, achieves this by applying the $L^2$-norm on the network weights, encouraging smaller weights \citep{gneccoWeightdecayTechniqueLearning2009}. Another approach described by \cite{kukackaRegularizationDeepLearning2017} is weight smoothing which applies $L^2$-norm to the gradients during training. Weight elimination is similar to weight decay but favours sparse networks \citep{weigendGeneralizationWeightEliminationApplication1990}. Soft weight-sharing clusters weights together, ensuring that weights within a cluster have similar values \citep{nowlanSimplifyingNeuralNetworks1992}.
	
	\cite{tianComprehensiveSurveyRegularization2022} discuss sparse vector-based regularisation, which imposes constraints on the activations. This is useful for applications requiring sparse representations, such as data compression. Continuing in the line of activation regularisation, \cite{tomczakLearningInformativeFeatures2016} introduces a regularisation term that encourages activations to maximise entropy, resulting in uncorrelated and disentangled representations. Meanwhile, \cite{wuImprovingInterpretabilityRegularization2018} introduce a regularisation term putting constraints on the output activations to improve the interpretability of representations.


\section{Alternative priors and posteriors in VAEs} \label{cha:rel_alt_priors}
	Taking inspiration from VAEs, V-GIM minimises the KL-divergence with its posterior distribution and a fixed prior $\prior=\standardnormal$. This results in a latent space that can be better understood. In recent years, several contributions have been made to VAEs concerning different priors or posteriors.
	
	In VAEs, the posterior $\qphizx$ serves as an approximation to the true posterior $\pphizx$ \citep{odaiboTutorialDerivingStandard2019}. This approximate posterior is most commonly chosen as a simple factorised Gaussian for mathematical convenience. However, this is often an oversimplification of the true posterior \citep{nalisnickApproximateInferenceDeep2016a}. \cite{kingmaIntroductionVariationalAutoencoders2019} demonstrate that the approximate posterior can be extended to a Gaussian with a full covariance matrix. Additionally, \cite{nalisnickApproximateInferenceDeep2016a} propose a Gaussian mixture model, which combines several Gaussians, as an approximate posterior, enabling the modelling of multimodal posterior distributions.
	
	Continuing in the exploration of Gaussian mixture models, alternative priors have also been investigated. \cite{guoVariationalAutoencoderOptimizing2020} and \cite{leeMetaGMVAEMixtureGaussian2021} experiment with Gaussian mixture model priors, resulting in improved performance. Additionally, \cite{tomczakVAEVampPrior2018} introduce VampPrior, choosing the prior as a mixture of variational posteriors. This approach improves performance and mitigates issues related to useless dimensions, which is a well-known problem in VAEs and is also observed in V-GIM.
	

------------------------

Understanding/Explaining Deep Neural Networks for speech:
- Visualizing Automatic Speech Recognition – Means for a Better Understanding? \citep{markertVisualizingAutomaticSpeech2021} % ze gebruiken salience map op spectrogram. ziet voor welke frequencies gevoel is. maar geeft dus eig maar weinig inzichten. laatstaan als ze dat op de ruwe speech waves doen.

- krugAnalyzingVisualizingDeep2021 : SNAPS for visualising data that is not supposed to be visualised such as audio.
	"Using spectrograms, it is possible to use saliency map methods because spectrograms allow experts with domain knowledge to visually interpret audio, as well [33–35]" % --> ik kan hun referneces gebruiken


------ post hoc explainability techniques (copied from introduction)

To gain a better understanding of these less interpretable models, various post-hoc approaches have been explored. These approaches aim to find explanations for models that are not interpretable by design, for instance through techniques such as visual explanations, text explanations and feature relevance explanations \citep{barredoarrietaExplainableArtificialIntelligence2020a}. These approaches can be categorised as ``model-agnostic", which find explanations irrespective of the model type, or ``model-specific", which make assumptions about the underlying workings of the model, for instance by analysing the activations of a neural network.

Two well-known agnostic model approaches are SHapley Additive exPlanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME). These approaches aim to explain individual predictions by assigning a positive or negative contribution score to each input feature with respect to the output prediction \cite{lundbergUnifiedApproachInterpreting2017, molnarInterpretableMachineLearning2022}. High contribution scores indicate features that have a significant impact, enabling humans to analyse which features the model found important.

SHAP achieves this through ... % TODO

Meanwhile, LIME determines its relevance scores by training an interpretable model that approximates the original model. This is done by creating a dataset consisting of perturbed input samples and their corresponding predicted output  \citep{ribeiroWhyShouldTrust2016}. The interpretable model can for instance be a decision tree \citep{molnarInterpretableMachineLearning2022}.

While model-agnostic approaches do not make assumptions on the black-box model, model-specific approaches do, and can therefore exploit more information resulting in more efficient or potentially more accurate explanations.

Saliency maps, also known under the names pixel attribution maps or sensitivity maps, just like SHAP and LIME are used to obtain a correspondence score for each input feature. However,

In particular, for convolutional neural networks, \cite{simonyanDeepConvolutionalNetworks2014} obtain the relevance scores by computing a gradient of predicted class with respect to the input features (or in their case pixels). The result is a number of partial derivatives for each pixel indicating how much the score will increase (or decrease) given changes from the image. From this gradient, a saliency map or heatmap can be constructed highlighting the important areas that led to a decision. 




Model-specific:
Saliency maps:
- Gradient only: Grad-CAM for convolutional neural networks "If I were to increase the color values of the pixel, the predicted class probability would go up (for positive gradient) or down (for negative gradient). The larger the absolute value of the gradient, the stronger the effect of a change of this pixel." - book
- gradient salancy: neural networks
its a map and highlights different regions on images
saliency: how important pixel was
- integraded gradients: look at training gradients as gives importance
- attention
-----------------------------















%We have studied the latent representations obtained from maximising the InfoNCE objective. 
%We achieved this through the introduction of V-GIM, a self-supervised representation learning approach with the same InfoNCE objective, but with an additional constraint to the latent space resulting in better interpretable representations. Such that a decoder could be trained and predict meaningful ...



%\section{Explainable AI}
%	%While multiple XAI techniques exist, they work in different paradigms, usually attempt to visualise 
%	This is a vastly different approach from existing techniques in explainable AI. Bai et al. group the techniques in three categories \citep{baiExplainableDeepLearning2021}; attribution-based methods, non-attribution-based methods and uncertainty quantification.
%	1) tries to attribute a prediction to its input features. eg used for images and can highlight regions contribute to the decision.
%	
%	
%	
%	X et al. group XAI techniques in x cateogories
%		In the field of Explainable AI multiple paradigms exist, ranging from activation heatmaps ...
%	These techniques give insights in visual domain, but lack in other domains such as speech domain where heatmaps be harder to gain insights from.


%	
%	- 
%
%---
%Explainable ANNs:
%	- diff paradigms, by looking at heath maps and lr etc
%	- our work is in fact new paradigm, by adding constraints to the optimisation metric, resulting in better understandable latent representations
%	- Explainable deep learning methods survey: \citep{baiExplainableDeepLearning2021} (attribution and non attribution, zie mijn draft.dox) --> probeert contribution van elke feature te linken. dat zijn technieken die werken voor foto's of feature vectors, maar voor puur sequential audio is moeilijker bruikbaar.
%	
%	Maybe exists other techniques that change the ANN resulting in better explainable. (eg pruning?)
%	- methodology to remove features that do not contribute to accuracy. (feature selection) with interpretability motivations. \citep{glorfeldMethodologySimplificationInterpretation1996}
%	
%	
%Explainable learning in speech data
%
%
%Summarise: our method: sequential data/speech data, interpretable, representation learning, disentanglement
