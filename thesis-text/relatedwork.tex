\chapter{Relation to Existing Work} \label{cha:5}
We have studied the representations obtained by optimising the InfoNCE loss function. Through the addition of a regularisation term to this loss function, we could analyse the underlying information contained in the representations through a decoder. In this chapter, we give an overview of the existing literature that is relevant to our research. We begin with a discussion on the more recent representation learning techniques related to mutual information (MI) maximisation. We give a non-comprehensive overview of the existing regularisation techniques used for both improved generalisation and better representations. We give an overview of alternative priors that have been introduced to the VAE framework in the past decade. Finally, we conclude this chapter with an overview of the well-known post-hoc explainability techniques and compare them against V-GIM's decoder.

%We have studied the representations obtained by optimising the InfoNCE loss function. Through the addition of a regularisation term to this loss function, we could analyse the underlying information contained in the representations through a decoder. In this chapter, we give an overview of the existing literature that is relevant to our research. We begin with a discussion of representation learning techniques related to mutual information (MI) maximisation and slowly digress into interpretable representations. Furthermore, we give an overview of the existing regularisation techniques used for both improved generalisation and better representations. Finally, we give an overview of alternative priors that have been introduced to the VAE framework in the past decade.



\section{Mutual Information and Interpretable Representation Learning}
	V-GIM is based on the ideas of MI maximisation introduced in CPC and GIM \citep{lowePuttingEndEndtoEnd2020a, oordRepresentationLearningContrastive2019}. V-GIM achieves this by maximising the MI between temporally nearby patches, assuming common information between nearby data \citep{lowePuttingEndEndtoEnd2020a}. This is achieved by optimising for the InfoNCE bound, a lower bound on the mutual information \cite{oordRepresentationLearningContrastive2019}. However, similar approaches utilising MI maximization for representation learning have been explored before. Deep InfoMax (DIM) \citep{hjelmLearningDeepRepresentations2019} incorporates an ANN encoder which maximises the MI between input and output. This is achieved by incorporating knowledge about locality in the representations, resulting in locally-consistent information across structural locations. 
	
	MI is notoriously difficult to compute directly, and is therefore frequently estimated instead \citep{tschannenMutualInformationMaximization2020}. \cite{pooleVariationalBoundsMutual2019} provide a taxonomy on this matter. Both DIM and CPC (and therefore also GIM and V-GIM), belong to the class of MI estimators through finding a lower bound on MI that can then be optimised without having to directly optimise MI. CPC, GIM and V-GIM utilise the InfoNCE bound, while DIM utilises Mutual Information Neural Estimation (MINE) \citep{belghaziMINEMutualInformation2021} which is based on the Donsker-Varadhan representation \citep{donskerAsymptoticEvaluationCertain1976}. Meanwhile, InfoGAN \citep{chenInfoGANInterpretableRepresentation2016} aims to estimate the MI directly by taking a variational approach; choosing an approximate posterior from which the MI can be approximated. The MI estimation is then added as a regularisation term to a GAN-like loss function, resulting in interpretable representations that can be used to influence individual of the generated data. For instance, manipulating an individual dimension in the latent space may influence the brightness in an image or the pose of a person. The generator in InfoGAN's GAN-like architecture is thus similar to how V-GIM achieves interpretability using the optional decoder. However, V-GIM takes this a step further by dividing its architecture into one or more modules, each with its own interpretable representations. This enables the understanding of the final representations, but also the internal parts of the neural network, providing increased interpretability.
	Similar to V-GIM, Contrastively Disentangled Sequential Variational Autoencoder (C-DSVAE) takes a contrastive approach on estimating a lower bound for MI, based on ideas in CPC. C-DSVAE is based on a sequential data, and can therefore also be trained on sequential data, just like V-GIM. However, V-GIM does not require the decoder for training. C-DSVAE's function optimises the log-likelihood (or reconstruction error) introduced in VAEs and considers the contrastive estimate on MI as an additional regularisation term. Meanwhile, V-GIM considers MI maximisation as main learning function while the KL-divergence is used as regularisation term. Additionally, C-DSVAE uses a different approach on deciding which samples are considered positive and negative samples \cite{baiContrastivelyDisentangledSequential2021}.

%	Meanwhile, C-DSVAE and S3VAE continue from the seqential VAE framework and each introduce a different flavour on the MI maximisation scheme, while also obtaining disentangled representations \citep{baiContrastivelyDisentangledSequential2021, zhuS3VAESelfSupervisedSequential2020}.
	
%	1) via contrstive estimation?
%	similar to V-GIM: To our knowledge, we are the first to synergistically combine the contrastive estimation and sequential generative models in a principled manner for learning disentangled representations.
	However, V-GIM does not require a model a decoder for training, the loss functions are also entirely different, as MI maximisation is what obtains performance, while their approach maximises the log likelihood (or reconstruction error) in VAEs. And the contrastive estimate on MI is considered as an additional regularisation term.
		%"Our method seeks to achieve clean disentanglement of s and z1:T , by optimizing the following objective function, which introduces additional MI terms to the vanilla ELBO"
		%Note that in CPC, for each frame, the positive example is a nearby frame. That is, CPC uses the temporal smoothness as the inductive bias for learning per-frame representations. In our setup however, we treat the entire trajectory z1:T as samples in contrastive estimation, and we must resort to other inductive biases for finding positive examples.
		% so they use different approach on on which samples are considered positive and negative samples
	
	% 2) also sequential vae
	
		% for disentanglement later: Î²-VAE [24] imposed a heavier penalty on the KL divergence term for a better disentanglement learning. Follow-up researches [33, 6] derived a Total Correlation (TC) from the KL term, and highlights this TC term as the key factor in disentangled representation learning.


	
	
	
	Building upon InfoGAN's concepts, Bridge-GAN introduces an intermediate latent space, or ``bridge" between text and images. This approach enables the synthesis of interpretable and disentangled representations for text-to-image synthesis \citep{yuanBridgeGANInterpretableRepresentation2020a}. Meanwhile, InfoVAEGAN's representations are learned by combining the framework of Generative Adversarial Networks with concepts from VAEs, enabling the learning of interpretable data variations \citep{yeInfoVAEGANLearningJoint2021}. Similar to V-GIM, latent representations are samples from a distribution, such that a single data point can have multiple latent representations.
	
	%Continuing in the line of interpretable representation learning, Timeline uses recurrent neural networks with an attention mechanism to aggregate sequential health data to interpretable representations \citep{baiInterpretableRepresentationLearning2018}. Its interpretability is achieved by analysing of the weights associated with different medical codes. Similarly, \cite{agrawalInterpretableRepresentationLearning2020} apply relevance weighting on raw speech data, allowing for interpretation of the representations during forward propagation. 
	
	
	

\section{Regularisation}
	The practice of adding a penalty term to the loss function, known as regularisation, has been widely used for various purposes. \cite{kukackaRegularizationDeepLearning2017} provide a survey and group the regularisation terms in different categories, including those that impose constraints on the weights, or on the activations. V-GIM belongs to the second category, applying constraints to the activations in different layers of the network.
	
	Regularisation terms that enforce constraints on the weights typically aim to improve generalisation performance by penalising complexity. Weight decay, for example, achieves this by applying the $L^2$-norm on the network weights, encouraging smaller weights \citep{gneccoWeightdecayTechniqueLearning2009}. Another approach described by \cite{kukackaRegularizationDeepLearning2017} is weight smoothing which applies $L^2$-norm to the gradients during training. Weight elimination is similar to weight decay but favours sparse networks \citep{weigendGeneralizationWeightEliminationApplication1990}. Soft weight-sharing clusters weights together, ensuring that weights within a cluster have similar values \citep{nowlanSimplifyingNeuralNetworks1992}.
	
	\cite{tianComprehensiveSurveyRegularization2022} discuss sparse vector-based regularisation, which imposes constraints on the activations, encouraging activations to approximate zero. This is useful for applications that require sparse representations, such as data compression. Continuing in the line of activation regularisation, \cite{tomczakLearningInformativeFeatures2016} introduces a regularisation term that encourages activations to maximise entropy, resulting in uncorrelated and disentangled representations. Meanwhile, \cite{wuImprovingInterpretabilityRegularization2018} introduce a regularisation term that puts constraints on the output activations to improve the interpretability of representations. In different work, \cite{wuOptimizingInterpretabilityDeep2021} propose a different approach to improve the interpretability of ANNs. This is achieved by constraining the depth required to approximate the model through a decision tree. The depth of the equivalent decision tree is added as a regularisation term to the loss function. Similarly to V-GIM, \citeauthor{wuOptimizingInterpretabilityDeep2021} aims to achieve better interpretability through an additional regularisation term while maintaining the original objective of the model. However, this approach is limited by the modelling capabilities of decision trees, as it can only be applied to supervised tasks. Consequently, it cannot be used for generative modelling or representation learning. Additionally, it is not well-suited for computer vision problems or speech recognition, as these tasks are difficult to model using decision trees \citep{stanfordmedaiMedAI34Optimizing2022}. Meanwhile, V-GIM does not suffer from these issues.
	
	

\section{Alternative Priors and Posteriors in VAEs} \label{cha:rel_alt_priors}
	Taking inspiration from VAEs, V-GIM minimises the KL-divergence with its posterior distribution and a fixed prior $\prior=\standardnormal$. This results in a latent space that can be better understood. However, other possibilities have been suggested in the literature on VAEs concerning different priors or posteriors.
	
	In VAEs, the posterior $\qphizx$ serves as an approximation to the true posterior $\pphizx$ \citep{odaiboTutorialDerivingStandard2019}. This approximate posterior is most commonly chosen as a simple factorised Gaussian for mathematical convenience. However, this is often an oversimplification of the true posterior \citep{nalisnickApproximateInferenceDeep2016}. \cite{kingmaIntroductionVariationalAutoencoders2019} demonstrate that the approximate posterior can be extended to a Gaussian with a full covariance matrix. Additionally, \cite{nalisnickApproximateInferenceDeep2016} propose a Gaussian mixture model, which combines several Gaussians, as an approximate posterior, enabling the modelling of multimodal posterior distributions.
	
	Continuing in the exploration of Gaussian mixture models, alternative priors have also been investigated. \cite{guoVariationalAutoencoderOptimizing2020} and \cite{leeMetaGMVAEMixtureGaussian2021} experiment with Gaussian mixture model priors, resulting in improved performance. Additionally, \cite{tomczakVAEVampPrior2018} introduce VampPrior, choosing the prior as a mixture of Gaussian posteriors. This approach improves performance and mitigates issues related to useless dimensions, which is a well-known problem in VAEs and is also observed in V-GIM (see section \ref{cha:bg_repr_analy}). In section \ref{cha:disc_repr_anal}, we make recommendations for future work on how these alternative priors or posteriors can be incorporated into V-GIM to deal with these useless dimensions.
	

\section{Post-Hoc Explainability}
Black-box models (including ANNs) are limited by their lack of interpretability. Consequently, different post-host approaches have been introduced, which aim to explain the predictions these models make. These approaches can be categorised as ``model-agnostic", which find explanations irrespective of the model type, or ``model-specific", which make assumptions about the underlying workings of the model, for instance by analysing the activations of a neural network \cite{barredoarrietaExplainableArtificialIntelligence2020a}.

Two well-known agnostic model approaches are Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) \citep{ribeiroWhyShouldTrust2016, lundbergUnifiedApproachInterpreting2017}. These approaches aim to explain individual predictions by assigning a positive or negative contribution score to each input feature with respect to the output prediction \cite{lundbergUnifiedApproachInterpreting2017, molnarInterpretableMachineLearning2022}. High contribution scores indicate features that have a significant impact, enabling humans to analyse which features the model found important. LIME determines its relevance scores by training an interpretable model that approximates the original model. The interpretable model is trained with predictions from the original model around a particular data point \citep{ribeiroWhyShouldTrust2016}. The interpretable model can for instance be a decision tree \citep{molnarInterpretableMachineLearning2022}. Meanwhile, SHAP obtains its contribution scores based on the Shapely values from coalitional game theory. This enables global interpretation based on the aggregation of the Shapely values \citep{molnarInterpretableMachineLearning2022}.

While model-agnostic approaches do not make assumptions on the black-box model, model-specific approaches do, and can therefore exploit more information resulting in more efficient and potentially more accurate explanations. Saliency maps, also known under the names: pixel attribution maps and sensitivity maps, similar to SHAP and LIME are used to obtain a correspondence score for each input feature \citep{molnarInterpretableMachineLearning2022}. In particular, for convolutional neural networks, \cite{simonyanDeepConvolutionalNetworks2014} obtain the relevance scores by computing a gradient of the predicted class with respect to the input features (or in their case pixels). The result is a series of partial derivatives, indicating how much the class score will increase (or decrease) given changes to its respective pixel. From this gradient, a saliency map or heatmap can be constructed highlighting the important areas that led to a decision. This idea has also been introduced under the name of Grad-CAM \citep{selvarajuGradCAMVisualExplanations2020}. 

\cite{erhanVisualizingHigherLayerFeatures2009} and \cite{simonyanDeepConvolutionalNetworks2014} suggest using gradient ascent to find the image that maximally activates a particular neuron, or output class, respectively. V-GIM's decoder can achieve a similar result without needing to apply gradient ascent for each neuron. After the decoder has converged, a latent representation with a maximum activation can be inserted in the decoder of which the original image can be directly decoded. In addition, since V-GIM constraints the latent space to be standard normal it is known up front what the expected range of activations is. The space constraints also ensure that the generated data is more likely to result in data points similar to the dataset, which are potentially easier to analyse. This is different from the results in \cite{erhanVisualizingHigherLayerFeatures2009, simonyanDeepConvolutionalNetworks2014}, where it is not always clear what is shown in the images as the models were trained without additional regularisation terms. 






%(van die YT VID) + book: https://christophm.github.io/interpretable-ml-book/neural-networks.html
%What about internals (look at neural representations):
%Our method is more like this approach, both explaining internals
%- feature visualisation
%focus on specific neuron, and use it to generate images to activate it.
%use the behaviour of that model and find image that activates the neuron.
%-> isolate neurons that detect textures etc.
%(distill.pub/2017/feature-visualisation, distill/2021/multimodal-neurons)
%
%
%
%%Additionally, researchers have explored methods to explain black box models.
%%- Relevance weighting, ... 

%^^ hun prentjes die ze genereren die de maximum activatie generenen, zijn geen echte prentjes, en zou wrs niet even goed interpreteerbaar zijn voor audio data, vermits her oor gevoeliger is voor disturbance en dus minder goed kan verstaan.
%*******************************************************************************









%We have studied the latent representations obtained from maximising the InfoNCE objective. 
%We achieved this through the introduction of V-GIM, a self-supervised representation learning approach with the same InfoNCE objective, but with an additional constraint to the latent space resulting in better interpretable representations. Such that a decoder could be trained and predict meaningful ...



%\section{Explainable AI}
%	%While multiple XAI techniques exist, they work in different paradigms, usually attempt to visualise 
%	This is a vastly different approach from existing techniques in explainable AI. Bai et al. group the techniques in three categories \citep{baiExplainableDeepLearning2021}; attribution-based methods, non-attribution-based methods and uncertainty quantification.
%	1) tries to attribute a prediction to its input features. eg used for images and can highlight regions contribute to the decision.
%	
%	
%	
%	X et al. group XAI techniques in x cateogories
%		In the field of Explainable AI multiple paradigms exist, ranging from activation heatmaps ...
%	These techniques give insights in visual domain, but lack in other domains such as speech domain where heatmaps be harder to gain insights from.


%	
%	- 
%
%---
%Explainable ANNs:
%	- diff paradigms, by looking at heath maps and lr etc
%	- our work is in fact new paradigm, by adding constraints to the optimisation metric, resulting in better understandable latent representations
%	- Explainable deep learning methods survey: \citep{baiExplainableDeepLearning2021} (attribution and non attribution, zie mijn draft.dox) --> probeert contribution van elke feature te linken. dat zijn technieken die werken voor foto's of feature vectors, maar voor puur sequential audio is moeilijker bruikbaar.
%	
%	Maybe exists other techniques that change the ANN resulting in better explainable. (eg pruning?)
%	- methodology to remove features that do not contribute to accuracy. (feature selection) with interpretability motivations. \citep{glorfeldMethodologySimplificationInterpretation1996}
%	
%	
%Explainable learning in speech data
%
%
%Summarise: our method: sequential data/speech data, interpretable, representation learning, disentanglement
