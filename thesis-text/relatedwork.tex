
%*******************************************************************************
\chapter{Relation to existing work}

We have studied the representations obtained by maximising the InfoNCE loss function. We achieved this through the addition of a regularisation term to the loss function, resulting in a space that is easier to analyse and understand. In this chapter, we give an overview of the existing literature that is relevant to our research. We begin with a discussion of representation learning techniques related to mutual information maximisation, and slowly digress into interpretable representations. Furthermore, we give an overview of the existing regularisation techniques used for both improved generalisation and better representations. Finally, we give an overview of alternative priors that are have been introduced to the VAE-framework.


\section{Mutual information and interpretable Representation learning}
	V-GIM is based on mutual information maximisation ideas introduced in CPC and GIM \cite{lowePuttingEndEndtoEnd2020, oordRepresentationLearningContrastive2019}. This is achieved by maximising the mutual information between temporally nearby patches, assuming common information between nearby data \cite{lowePuttingEndEndtoEnd2020}. However, similar mutual information maximisation for representation learning ideas have been explored as well. Deep InfoMax considers an ANN encoder model which maximises the mutual information between input and output. This is achieved by incorporating knowledge about locality in the representations, resulting in locally-consistent information across structural locations \cite{hjelmLearningDeepRepresentations2019}.
	C-DSVAE learns disentangled representations for sequential data. Similar to V-GIM, this is achieved by maximising the mutual information between inputs and outputs, while separating static and dynamic factors in the latent space \cite{baiContrastivelyDisentangledSequential2021}. S3VAE uses a sequential VAE which learns disentangled representations for Data Generation \cite{zhuS3VAESelfSupervisedSequential2020}.
	InfoGAN \cite{chenInfoGANInterpretableRepresentation2016} is an extension to GANs, learning representations by maximising the mutual information between the generated data and small subsets of the latent variables in the latent representations. In addition, InfoGAN's learning approach results in disentangled and interpretable representations, where manipulating an individual dimension in the latent space, results in changes to a specific feature of the generated data, leaving other features unaffected. This is achieved through a modification of the traditional minimax loss function in GANs. Similarly, Bridge-GAN builds upon these concepts by introducing an intermediate latent space, or "bridge," between text and images, resulting in interpretable and disentangled representations for text-to-image synthesis \cite{yuanBridgeGANInterpretableRepresentation2020}.
	
	Further continuing in representation learning methods which incorporate an interpretability mechanisms, Timeline uses RNNs with an attention mechanism to aggregate sequential health to interpretable representations \cite{baiInterpretableRepresentationLearning2018}. This interpretability is achieved through analysis of weights associated to different medical codes. Similarly, Agrawal and Ganapathy use relevance weighting on raw speech data, allowing for interpretation of the representations during forward propagation \cite{agrawalInterpretableRepresentationLearning2020}. Similar to V-GIM, InfoVAEGAN combines the GAN framework with concepts from VAEs, enabling the learning of interpretable data variations \cite{yeInfoVAEGANLearningJoint2021}.
	
	

\section{Regularisation}
	 The idea of adding a penality to the loss function has been commonly used in the past with a wide variety of purposes. Kukačka and Golkov provide a survey and sort the regularisation terms under different categories, introducing regularisation terms which pose constraints on the weights, and a second category that that puts constraints on the activations \cite{kukackaRegularizationDeepLearning2017}.
	
	Regularisation terms which enforce constraints on the weights typically aim to improve generalisation performance by penalising complexity. Weight decay is such an example, enforcing smaller weights achieved through the $l^2$-norm on the network weights \cite{gneccoWeightdecayTechniqueLearning2009}. Similarly, Kukačka and Golkov describe weight smoothing which applies $l^2$-norm to the gradients during training. Weight elimination is similar to weight decay but favours sparse networks \cite{weigendGeneralizationWeightEliminationApplication1990}. Soft weight-sharing clusters weights together such that weights in a cluster share similar values \cite{nowlanSimplifyingNeuralNetworks1992}.
	
	Tian and Zhang describes sparse vector-based regularization, posing constraints on the activations. This is useful for applications which require sparse representations such as data compression \cite{tianComprehensiveSurveyRegularization2022}. Further continuing in the line of activation regularisation, Tomczak introduce a regularisation term to encourages activations to maximise entropy, resulting in uncorrelated and disentangled representations \cite{tomczakLearningInformativeFeatures2016}. Meanwhile, Wu et al. introduce a regularisation term putting constraints on the output activations to improve interpretability of representations \cite{wuImprovingInterpretabilityRegularization2018}.


\section{Alternative priors and posteriors in VAEs}	% TODO: MAYBE THIS SHOULD BE MOVED TO DISCUSSION/FUTURE WORK?
	Taking inspiration from VAEs, V-GIM aims to minimise the KL-divergence with its posterior distributions $\qphizx$ and prior $\prior=\standardnormal$. Doing so results in a latent space which can be better understood. In recent years multiple contributions were introduced to VAEs in relation to different priors and posteriors.
	
	In VAEs the posterior $\qphizx$ is in fact an approximate for the true posterior $\pphizx$ \cite{odaiboTutorialDerivingStandard2019}. This approximate posterior is most commonly chosen to be a simple factorised Gaussian for easier mathematical derivations, however, this is typically an oversimplification of the true posterior \cite{nalisnickApproximateInferenceDeep}. Consequently, Kingma and Welling show that the approximate posterior can be extended to a Gaussian with full covariance matrix \cite{kingmaIntroductionVariationalAutoencoders2019}. Additionally, Nalisnick et al. propose a Gaussian mixture model (a combination several Gaussians) as approximate posterior, enabling multimodal posterior distributions \cite{nalisnickApproximateInferenceDeep}.
	
	Continuing in the line of Gaussian mixture models, extensions to alternative priors have been explored as well. Lee et al. and experiment with Gaussian mixture model priors \cite{leeMetaGMVAEMixtureGaussian2021, guoVariationalAutoencoderOptimizing2020}, resulting in improved performance. Additionally, Tomczak and Welling introduce VampPrior, which choses the prior to be a mixture of variational posteriors, resulting in improved model performance and reducing issues related to useless dimensions \cite{tomczakVAEVampPrior2018}. This is a known issue in VAEs which we observed V-GIM suffers from as well.









%We have studied the latent representations obtained from maximising the InfoNCE objective. 
%We achieved this through the introduction of V-GIM, a self-supervised representation learning approach with the same InfoNCE objective, but with an additional constraint to the latent space resulting in better interpretable representations. Such that a decoder could be trained and predict meaningful ...



%\section{Explainable AI}
%	%While multiple XAI techniques exist, they work in different paradigms, usually attempt to visualise 
%	This is a vastly different approach from existing techniques in explainable AI. Bai et al. group the techniques in three categories \cite{baiExplainableDeepLearning2021}; attribution-based methods, non-attribution-based methods and uncertainty quantification.
%	1) tries to attribute a prediction to its input features. eg used for images and can highlight regions contribute to the decision.
%	
%	
%	
%	X et al. group XAI techniques in x cateogories
%		In the field of Explainable AI multiple paradigms exist, ranging from activation heatmaps ...
%	These techniques give insights in visual domain, but lack in other domains such as speech domain where heatmaps be harder to gain insights from.


%	
%	- 
%
%---
%Explainable ANNs:
%	- diff paradigms, by looking at heath maps and lr etc
%	- our work is in fact new paradigm, by adding constraints to the optimisation metric, resulting in better understandable latent representations
%	- Explainable deep learning methods survey: \cite{baiExplainableDeepLearning2021} (attribution and non attribution, zie mijn draft.dox) --> probeert contribution van elke feature te linken. dat zijn technieken die werken voor foto's of feature vectors, maar voor puur sequential audio is moeilijker bruikbaar.
%	
%	Maybe exists other techniques that change the ANN resulting in better explainable. (eg pruning?)
%	- methodology to remove features that do not contribute to accuracy. (feature selection) with interpretability motivations. \cite{glorfeldMethodologySimplificationInterpretation1996}
%	
%	
%Explainable learning in speech data
%
%
%Summarise: our method: sequential data/speech data, interpretable, representation learning, disentanglement
