
%*******************************************************************************
\chapter{Related work}
We have studied the latent representations obtained from maximising the InfoNCE objective. 
We achieved this through the introduction of V-GIM, a self-supervised representation learning approach with the same InfoNCE objective, but with an additional constraint to the latent space resulting in better interpretable representations. Such that a decoder could be trained and predict meaningful ...
This is a vastly different approach from existing techniques in explainable AI.

In the field of Explainable AI multiple paradigms exist, ranging from activation heatmaps ...
These techniques give insights in audio domain, but lack in other domains such as speech domain where heatmaps be harder to gain insights from.


\section{Representation learning: explainable}
\section{Variational learning}
There already were a few papers with variational contrastive predictive coding

\section{Links I should investigate}

- Adversarial and Contrastive Variational Autoencoder for Sequential Recommendation
https://arxiv.org/pdf/2103.10693.pdf

- S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation
https://arxiv.org/abs/2005.11437

- Implementation of Sequential VAE
https://github.com/ermongroup/Sequential-Variational-Autoencoder

- Contrastively Disentangled Sequential Variational Autoencoder
https://proceedings.neurips.cc/paper/2021/file/53c5b2affa12eed84dfec9bfd83550b1-Paper.pdf

- Sequential Variational Autoencoders for Collaborative Filtering
https://arxiv.org/pdf/1811.09975.pdf

- !! Variational noise contrastive estimation:
https://arxiv.org/abs/1810.08010