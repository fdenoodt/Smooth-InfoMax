\chapter{Variational InfoNCE}

\section{Motivation} %\section{Problem setting}
	In the previous section we discussed two categories of representation learning though deep learning. First, we discussed the autoencoder and its variational counterpart, which minimise the reconstruction error. Secondly, we discussed Contrastive Predictive Coding and Greedy InfoMax, both of which optimise the Info NCE objective. This category seeks to maximise the mutual information between the encodings of data patches that are temporally nearby. The latent representations obtained from all four methods can then be utilised for downstream tasks \cite{bengioRepresentationLearningReview2013, weiRecentAdvancesVariational2021, oordRepresentationLearningContrastive2019, lowePuttingEndEndtoEnd2020}
	
	% repr learn autoenc + vae (disentenglement)
		The autoencoder's sole objective is to define representations to reconstruct the original data. As a result, the representations may serve well for data compression, however, no additional constraints are enforced, such as feature disentanglement and thus the latent space may still be hard to work with for downstream tasks \cite{tschannenRecentAdvancesAutoencoderBased2018}. Meanwhile, VAEs' additional regularisation term, results in representations which break down or disentangle each feature into a narrowly defined variable and encodes them as separate dimensions \cite{weiRecentAdvancesVariational2021}. This additional constrained may result in better suited representations for downstream tasks. % TODO: I could reformulate this, and mention meta priors such as in this paper: https://arxiv.org/pdf/1812.05069.pdf

	% cpc contrasts noise -> smaller architect
		Both autoencoders and VAEs merely learn to reconstruct the data. Hence, all the "information" that is important to reconstruct the data will be maintained in the latent representation, whether the information is useful for the downstream task or not. Meanwhile, optimising latent representations for the InfoNCE objective will maintain shared information between temporally nearby patches, while discarding local noise. Reconstruction is thus not needed for training. This strategy has the tremendous benefit that a decoder block is not required, resulting in a significantly simplified architecture, meanwhile maintaining state-of-the-art performance \cite{stackeEvaluationContrastivePredictive2020}. A second benefit of these mutual information maximisation models is that they are directly compatible with sequential data.
		
	% lead to interpretabil
		Both categories (reconstruction and information maximisation algorithms) possess the ability to obtain useful representations for various downstream tasks. However, the content of these representations may not always be intuitive to humans and their structure may be difficult to comprehend. While CPC and GIM are considered state-of-the-art, their performance comes at a cost of having the least interpretable representations. Autoencoders maintain interpretability by using a decoder to reveal the information contained in the latent representation. The same transparency can also be achieved with VAEs. Additionally, by using a standard Gaussian as a prior and constraining the latent distributions to be similar to this prior, we can interpolate between representations and observe the effects through the decoder. As such, we can observe the specific information that is contained in each of the representation's features. VAEs can also result in disentangled features, further enhancing interpretability \cite{grossuttiDeepLearningInfrared2022}. In contrast, CPC and GIM do not contain a built in decoder mechanism, nor pose constraints on the latent space, significantly reducing interpretability.
		


\section{Towards decoupled training for probabilistic representations}
	% Our contribution
		In what follows next we introduce "Variational Greedy Infomax", maintaining the state-of-the-art performance obtained from optimising InfoNCE, while leveraging the interpretable and disentangled benefits from VAEs. This is achieved by optimising a novel loss function, \textit{Variational-InfoNCE}, a combination of InfoNCE and the regularisation term from VAEs. Additionally, by splitting up the neural network into modules, as introduced in \cite{lowePuttingEndEndtoEnd2020}, we greedily optimise each module with its own instance of this loss function. As a result, the interpretability benefits from VAEs will also be applicable in-between modules. This is in contrast to VAEs where solely the final output representations are interpretable.		
				
	% How:
		% still maximise mutual information between zt, ztk, but predictions no longer fixed datapoints.
		% xt -> cpc model -> q( . | xt) = mui, sigmai
		
		As discussed in the section on Contrastive Predictive Coding (CPC), a patch of sequential data $\xt$ is encoded through $g_{enc}(\xt) = \zt$ and aggregated over previous encodings through auto-regressor $g_{ar}(\z_1  \dots \zt) = \ct$, where both $\zt$ or $\ct$ may serve as representations for downstream tasks. The encoder function $\genc$ is represented as neural network, eg via a CNN, and $\gar$ for instance as a GRU. % todo: are gru and cnn used before?
		Finally, the encoding functions $\genc$ and $\gar$ are obtained by optimising a global loss function, the InfoNCE loss, end-to-end via backpropagation. 

		% split in modules
			Instead, in this study, we split up $\genc$'s network architecture by depth into $M$ modules 
			$$g_{enc}^1(\cdot),~ g_{enc}^2(\cdot),~\dots,~g_{enc}^M(\cdot)$$ 
			and prevent gradients from flowing between modules, as introduced in \cite{lowePuttingEndEndtoEnd2020}. An additional optional $M+1$'th module $\gar$ can be appended to the architecture. Each module is greedily optimised via a novel loss function, $\Lvnce$, which we will define in a following subsection. Each module's output serves as input for the successive module, as presented in the following equations, and depicted in figure \ref{fig:variationalgim}.
			
			\begin{figure} % fig: overview multiple modules
				\centering
				\includegraphics[width=0.7\linewidth]{temp_variational_gim}
				\caption{}
				\label{fig:variationalgim}
			\end{figure}
			
			\begin{align*} % g_enc1, ...
				g_{enc}^1(\xt) &= \zt^1 \\
				g_{enc}^m(\zt^{m-1}) &= \zt^m \\
				g_{ar}(\z_1^M ~ \dots ~ \zt^M) &= \ct
			\end{align*}
			
			The final representation $\ct$ is obtained by propagating $\xt$ through each modules as follows:
			$$ g_{ar}(g_{enc}^M ( \dots	g_{enc}^2(g_{enc}^1(\xt)))) $$
			
			% TODO: WANNEER OVER GRADIENTS BEGINT, A SINGLE MODULE IS DEFINED AS FOLLOWS.. MET F(z m-1) -> (mu, sigm)
			
			
					
		% Distributions
			Additionally, taking inspriation from VAEs, the outputs from $\gencm$ and $\gar$ are in fact samples from a distribution denoted by $q(\zt^m \mid \zt^{m-1})$, defined as a multivariate Gaussian with diagonal covariance matrix, as follows:
			$$\sampleqdot{\zt^{m-1}} = \normalfatmusigma$$
			with $\mufat$ and $\sigmafat$ dependent on $\zt^{m-1}$, specified in more detail in a following subsection.
			The outputs for $\gencm$ and $\gar$ are obtained by sampling from this distribution, denoted respectively, as follows:
 			\begin{align} % z ~ q AND c ~ q
			 	\sample{\zt^m}~ & \qfromzmneg  \label{eq:sample_z_from_q} \\
			 	\sample{\ct}~ & \qfromzM
			 \end{align}
			Modules are thus stochastic and computing $g_{enc}^m(\zt^{m-1})$ twice will likely result in two different representations of $\zt^m$. This is in contrast to CPC and GIM's encodings which remain fixed depending to the input \cite{oordRepresentationLearningContrastive2019, lowePuttingEndEndtoEnd2020}.
		
		% How distributions: predict q + sample
			We achieve these stochastic modules by defining each module $\gencm$ consisting of two blocks. The first block receives as input $\zt^{m-1}$ and predicts the parameters $\mufat$ and $\sigmafat$. These two parameters describe the distribution $\qfromzmneg$. Since we defined $q$ as Gaussian with a diagonal covariance matrix, the distribution can be fully described by those two vectors. The second block samples 
			$\sample{\zt^m} \sampleqdot{\zt^{m-1}}$ from this distribution and produces an output representation. This is depicted in figure \ref{fig:single_variational_module}.
			
			\begin{figure}[h] % img: single module
				\centering
				\includegraphics[width=0.7\linewidth]{temp_variational_module}
				\caption{}
				\label{fig:single_variational_module}
			\end{figure}
			
		% define ztm~q() = gaussian., --> z = mu + sigma*noise
			In practice, sampling from $q$ is achieved through a reparametrisation trick, as introduced in \cite{kingmaAutoEncodingVariationalBayes2022}. The equation to compute $\zt^m$ then becomes:
			\begin{equation*}
				\zt = \mufat + \sigmafat \odot \epilonfat
			\end{equation*}
			where $\epilonfat$ corresponds to a sampled value $\samplestandardnormal{\epilonfat}$ and $\odot$ is element-wise multiplication. The procedure to obtain $\ct$ is analogous to $\zt^m$ which we described above.
			
			
		Because of this probabilistic approach, a single patch of data $\xt$ will have multiple representations $\zt^M$, providing increased variance in the representations. This can potentially benefit downstream tasks, particularly when labelled data in scarce \cite{weiRecentAdvancesVariational2021}, leading to improved performance. % TODO: this benefit should be moved to benefits section.
			

		
		
\section{The learning objective}
	Instead of training the neural network end-to-end with a global loss function, the network is split up into modules, which each are optimised greedily with their own personal loss function. Through the introduction of the novel \textit{Variational-InfoNCE} loss, mutual information between temporally nearby representations is maximised, while regularising the latent space to be approximate to the standard Gaussian $\standardnormal$. The Variational-InfoNCE loss is defined as follows:
	
	\begin{equation} % variational_gim_loss % TODO: die k's is niet echt correct/onvolledig
		% \mathcal{L}(\ztk^{m-1}, \zt^{m-1}) = 
		\Lvnce^m =
		\underbrace{\reconstrgim}_{\text{Maximise } I(\ztk^m, \zt^m)} + \underbrace{\beta ~ \latentspaceconstraintgim}_{\text{Regularisation}}
		\label{eq:variational_gim_loss}
	\end{equation}

	$m \in \naturalset$ refers to the $m$'th module. $k \in \naturalset$ corresponds to the number of patches in the future the similarity score $\fkm$ must rate. $\ztk^m$ and $\zt^m$ are encoded samples produced by $g_enc^m(\ztk^{m-1})$ and $g_enc^m(\zt^{m-1})$, respectively. $X$ is a set of samples ${ \left\{ \ztk^m, \z_1^m, \z_2^m, \dots \right\} }$ where $\zj^m \neq \ztk^m$ are random samples.


	The similarity score $f_k^m(\cdot)$'s definition is identical to \cite{lowePuttingEndEndtoEnd2020}:
	
	$$ f_k^m(\ztk^m,\zt^m) = \exp({\ztk^m}^TW_k^m\zt^m) $$

	\subsection{Gradient}
		The gradient of the first term in $\Lvnce^m$ can be approximated through mini-batches, and optimised directly in PyTorch. With regards to the second term, since $\qfromzmneg$ is a Gaussian, a closed form solution exists \cite{kingmaAutoEncodingVariationalBayes2022}, the term can be differentiated without approximated method.
		
	
		%Where the KL divergence for a single sample $x^{(i)}$ is approximated as follows:
		% https://arxiv.org/pdf/1312.6114.pdf, from example
		\begin{equation}
			\frac{1}{2}\sum_{j=1}^J \left( 1 + \log((\sigma_j^{(i)})^2) - (\mu_j^{(i)})^2 - (\sigma_j^{(i)})^2 \right) 
		\end{equation}
	
		\begin{equation} % REAL BUT MUST REMOVE THE (i)	
			\kl{\normal}{\standardnormal} = \latentspaceconstraintclosedform
		\end{equation}
	
		
		where $z^(i,l) = \sigma ^{(i)} \odot \epsilon^{(l)}$ and $\epsilon^(l) \mathcal{N}(0, I)$
	
		\textbf{todo: variables should maybe be bold.}
		
		
	%	TODO: DUS DIE KL DIVERGENCE CLOSED FORM NOTATIE, MAAR BIJ SAMPLING ZOU OOK DEFINITIE VAN Z = MU + SIGMA*ERR GEVEN
		
	
	
	
\section{Practical benefits}

\subsection{Latent representations}
	Representations:
		Interpretability of latent representations:
			- Interpolation --> adds decoder
	
	Better generalisation for downstream tasks
		- Overfitting: reduction of required labelled data needed. Similar data is similar region, the kl divergence makes regions bigger.

		Overfitting during inference:
		- The same datapoint has multiple (similar) representations, such that learning techniques for downstream tasks will not be able to "memorise" the latent space as easily.
		
		- Holes: more predictable inference, such that unseen data is more likely to be near clusters. And thus downstream tasks receive latents that are more similar to what is seen before.
		= better generalisation
		
		% copied from a bit higher		
		% Because of this probabilistic approach, a single patch of data $\xt$ will have multiple representations $\zt^M$, providing increased variance in the representations. This can potentially benefit downstream tasks, particularly when labelled data in scarce \cite{weiRecentAdvancesVariational2021}, leading to improved performance. % TODO: this benefit should be moved to benefits section.

\subsection{Training}
	- Batch normalisation: is useful for following modules, BUT ALSO DOWNSTREAM TASKS!
	- GIM advantages remain: maintain the benefits such as smaller networks that can learn indep, ALSO NO EXPLICIT NEED FOR A DECODER. THIS REMAINS, SO ARCH CAN BE SIMPLER.
	

	- Independent latent dimensions
	- During training similar behaviour to batch normalization in-between layers







\textbf{Other sources:} \\
%!!! Abstract on VAE: The fundamental idea in VAEs is to learn the
%distribution of data in such a way that new meaningful data with more intra-class variations can be generated
%from the encoded distribution.
%The ability of VAEs to synthesize new data with more representation variance
%at state-of-art levels provides hope that the chronic scarcity of labeled data in the biomedical field can be
%resolved.
%--> and thus for downstream tasks, has a way of obtaining more labelled data? --> better generalisation


%The goal of representation learning is to be useful for downstream tasks. The most important meta-prior is called ‘disentanglement’ which is an unsupervised learning technique that breaks down, or disentangles, each feature into narrowly defined variables and encodes them as separate dimensions 

%Intuitively, a factorial code disentangles the individual elements that were originally mixed in the sample, just as
%humans recognize complex things by disentangling independent elements. If the dimensions of the latent vector are
%independent of each other, it is factorial disentangled, i.e., a
%good representation. VAEs have made such nonlinear latent
%variable models tractable for modeling complex distributions,
%and efficient extraction of relevant biological information
%from learned features for biological data sets, referred to as
%unsupervised representation learning
%https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9311619








%We show that the Beta-VAE outperforms principal component analysis (PCA) and learns interpretable and independent representations of the generative factors of variance in the spectra %https://pubs.acs.org/doi/pdf/10.1021/acs.jpclett.2c01328
%


