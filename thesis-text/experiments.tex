\chapter{Experiments}

In this chapter, we evaluate the effectiveness of the latent representations obtained from Variational Greedy InfoMax (V-GIM) in comparison to its non-variational counterpart, GIM, by training both models on sequential data from the audio domain. To assess the quality of the representations, we project them into a two-dimensional space using t-SNE and analyse the emergence of potential clusters. Moreover, we use a linear classifier that takes these representations as input, and evaluate its accuracy to gain insights into the performance of these models. To further examine the efficacy of these representations, we investigate their potential for downstream task generalisation by training the linear classifier on smaller subsets of the dataset and assessing the amount of labelled data required to achieve satisfactory performance. Finally, we delve into the underlying structure of V-GIM's representations by training a decoder on top of each of V-GIM's modules, providing insights into their composition.


%- Experimental details encoder:
%	- Learning task: to find representations for speech data.
%	- Dataset:
%		- full data + split up
%	- Architecture
%- Results
%	- Loss functies
%	- Linear separability (t-sne)
%	
%- Generalisation (generalisation)
%		- Accuracies Subsets
%- Interpretability
%		- Encoder distributions (shows that interpolation can make sense)
%		- Decoder: loss, architecture

	






% TODO: I CALL IT Variational Greedy Infomax: V-GIM

\section{Experimental details GIM and V-GIM} \label{cha:experim_details_vgim}
	% VGIM + GIM, trained on Bart's dataset, 80\%, 20 test set
		GIM and V-GIM are trained on raw speech signals of fixed length sampled at 16kHz. The dataset consists of 851  audio files and is randomly shuffled and split up into 80\% training data (680 files) and 20\% test data (171 files). Each audio file consists of a single spoken sound consisting of three consonants and three vowels, where the consonants and vowels alternate with each other. Some examples are the sounds ``gi-ga-bu" and ``ba-bi-gu". The words consist of three syllables from the following list: ba, bi, bu, da, di, du, ga, gi and gu. All the sounds are spoken by the same person, in a constant and peaceful tone. We crop the files to a fixed length of 10240 samples, or 640 milliseconds, which is slightly longer than half a second.
		
	% In contrast GIM's architecture introduced in LÃ¶we et al.
		GIM and V-GIM are trained on the same architecture, consisting of two encoder modules $g_{enc}^1(\cdot)$ and $g_{enc}^2(\cdot)$, no autoregressor module $\gar$ is used. Modules are trained in parallel. Each module consists of solely convolution and pooling layers. Since the architecture contains no fully connected layers, the length of the audio files can therefore be variable during inference. The architecture details for the two modules are presented in tables \ref{tab:architecure-module-1} and \ref{tab:architecure-module-2}. ReLu non-linearity is applied after each convolution, except in the last layer in each module. No batch normalisation is used. The number of hidden channels produced by each convolution and pooling layer remains constant, at 32 channels. In each module, data flows synchronously from one layer to the successive layer, except in the final two layers. These run in parallel and both take the same input from the preceding layer. This architecture choice is related to the parametrisation trick we discussed in section \ref{cha:vgim_decoupled_training_for_probabil_repr}. One layer generates $\mufat$ and the other $\sigmafat$ such that $\ztm$ can be computed via the reparametrisation trick. The architecture is visualised in figure \ref{fig:architecture}. 
		An input signal of length $1 \times 10240$ is downsampled to $32 \times 52$ by the first module and to $32 \times 13$ by the second module. Due to overlap, each latent representation $\zt^M = \zt^2$ captures $64$ms of speech. %Previously I wrote: Each latent representation $\zt^M = \zt^2$ captures $49$ms of speech. WRONG. this would be the case if there was no overlap. but single timestep t corresponds with 1024 samples. or 64 milliseconds.
		
		% As a result of this architecture, an input signal of length $1 \times 10240$ is downsampled to $32 \times 52$ by the first module and to $32 \times 13$ by the second module. Each latent representation $\zt^M = \zt^2$ captures $49$ms of speech.

	
		% Table
		\input{graphs/vgim_architecture}

		\begin{figure}
			\centering
			\hspace*{1.5cm}
			\begin{annotatedFigure}
				{\includegraphics[width=0.8\linewidth, trim={0 2.8cm 0 2cm}, clip]{"graphs/vgim architecture"} }
				
				
				\annotatedFigureText{0.1,0.15}{black}{0.31}{$\xt$}{8}
				
				\annotatedFigureText{0.565,0.28}{black}{0.31}{$\zt^1$}{8}
				\annotatedFigureText{0.88,0.3}{black}{0.31}{$\zt^2$}{8}
				
			\end{annotatedFigure}
			\caption{Visualisation of V-GIM's architecture.}
			\label{fig:architecture}
		\end{figure}
		
		
		Importantly, the reparametrisation trick is included in our implementation of GIM's architecture as well. However, by enforcing no constraints on the latent space, which is achieved by assigning $\beta = 0$ in $\Lvnce$, we observed that $\sigmafat$ moves towards \textbf{0} such that all the randomness from sampling from a Gaussian distribution is removed. As such, our implementation of GIM is equivalent to GIM introduced in \citep{lowePuttingEndEndtoEnd2020} but with an altered architecture.
		
	% - ADAM optimiser, decaying learning rate, batch size = 171 (= test data), 800 epochs
	% - 10 negative samples, predict k = 12 time steps in the future
		Both GIM and V-GIM are trained using the Adam optimiser for 800 epochs with an exponential learning rate scheduler \citep{bhargavladGuidePytorchLearning2020}, with initial learning rate $lr_{init} = 0.01$ and $\gamma=0.995$, such that $lr$ is updated as follows:
		$$lr_{epoch} = \gamma * lr_{epoch - 1}$$
		The batch size is set to 171, which is exactly the size of the test set. The number of patches to predict in the future $k$, is set to 12. Implementation details with regards to drawing negative samples for $\fkmblank$ remain identical to the experiments from \citep{oordRepresentationLearningContrastive2019} and \citep{lowePuttingEndEndtoEnd2020}. The regularisation importance term $\beta$ is set to 0 for GIM and 0.0035 for V-GIM, which is the largest value we could obtain without causing posteriors to collapse to the standard normal, commonly referred to as posterior collapse.

	%	% classifier:
	%	After convergence, both methods trained on multi class logistic regressor?	linear multi-class classifier
	
		After training V-GIM and GIM for 800 epochs, we train linear multi-class classifiers for 100 epochs on their respective latent representations for every module, evaluated using Cross-Entropy \citep{hoRealWorldWeightCrossEntropyLoss2020}. We set $lr=0.001$ and use the Adam optimiser without scheduler. The classifier is tasked to predict the syllable corresponding to its latent representation. Details on the algorithm used to split up audio files by individual syllables are provided in appendix \ref{appendix:split_syllables}. Speech waves are zero-padded at both the beginning and the end to achieve a fixed length between syllables. The corresponding latent representations thus all consist of a fixed number of time frames $\mathcal{T}$ and 32 channels. Here, $\mathcal{T}$ is different depending on the module. 
		The classifier consists of a single connected layer with $32 \times \mathcal{T}$ input nodes and 9 output nodes, matching the number of input features and number of classes respectively.  We reshuffle the dataset and use 80\% for training and 20\% for testing.

	

	\section{Results GIM and V-GIM}	
	\subsection{Training error} \label{cha:experiments_vgim_train_err}
	% 4 Loss functions (2 modules, gim+vgim)
		
		
		Figure \ref{fig:four_losses} displays the loss curves corresponding to GIM ($\beta=0$) and V-GIM ($\beta=0.0035)$ for both modules. In the first module, the test loss for GIM is lower compared to V-GIM. This is expected because V-GIM includes an additional regularisation term in its loss function. In the second module, both GIM and V-GIM have loss curves that converge to lower values than their previous module. This suggests that the task becomes easier for the subsequent modules, indicating that the preceding module does indeed make simplified representations of the speech data.
		
		Interestingly, in the second module, V-GIM has a lower loss compared to GIM, even though V-GIM includes the regularisation term in its loss function. We argue that this is related to internal covariate shift, which we discussed in section \ref{cha:vgim_batch_norm}. Using a relatively large learning rate for GIM and V-GIM leads to a significant ``drift" in the distribution of activations during training, making it difficult for GIM's successive modules to keep up with these changes. V-GIM is less susceptible to this issue due to the internal batch-normalisation mechanism that is built into the regularisation term. Consequently, V-GIM's modules can train with a larger learning rate without affecting the performance of successive modules, resulting in faster convergence. %In section \ref{cha:exper_classifier} we discuss the significant implications in performance for downstream tasks.
		
		\input{graphs/gim_vgim_losses}
		  



		
	\subsection{t-SNE} % TODO: moet single syllable uitleggen met opsplitsing
	We project the latent representations obtained from each module to a two-dimensional plane using t-SNE \citep{maatenVisualizingDataUsing2008}. This enables us to observe potential clusters, as similar data points are mapped close together, while dissimilar points remain far away. Similar to the classifier discussed in section \ref{cha:experim_details_vgim}, t-SNE is trained on flattened representations that are split into individual syllables, without performing any pooling. We run t-SNE with random initialisation, perplexity of 30 and a learning rate of 200 for 1000 iterations.
	
	The graphs for GIM and V-GIM are displayed in figure \ref{fig:four_tsne_plots}. T-SNE was not provided with any information about the class labels. Syllables containing the vowel ``a" are represented with a red tint, ``u" with green, and ``i" with blue. We observe similar results in the first module of both GIM and V-GIM. T-SNE identifies two clusters, with one cluster corresponding to syllables containing an ``a" and the other cluster to containing ``u" or ``i". Within the ``u/i" cluster, there is still a grouping of ``u" and ``i" data points. However, distinguishing between ``b", ``d" or ``g" is more challenging as data points within the clusters are entangled.
	
	In the second module, we observe more significant differences between GIM and V-GIM. In V-GIM's second module, there is a clearer separation between the ``i" and ``u" syllables, indicating that the second module contributes to improved latent representations. However, distinguishing between the pronounced consonant remains difficult. On the other hand, GIM's second module does not appear to have converged well, as t-SNE struggles to separate the representations into meaningful clusters. Consequently, syllables are mixed without much structure. This further emphasises how GIM is affected by internal covariate shift, while V-GIM is not.
	
	\input{graphs/four_tsne_plots}
	
	%TODO: DIT KAN IN CONCLUSIE MIS HERGEBRUIKT WORDEN
	%\ref{fig:tsne_two_module_kld_0}: can see that the second model harms performance. We believe this can be explained via the learning rate. \ref{fig:tsne_two_module_kld_0033}, there we see that the second module performs better separation, indicating that the intermediate KL convergence constraint (causing the normal Gaussian distributions) also serves as a batch normalisation term, and thus resulting in faster convergence.
		
	
	\subsection{Classification performance} \label{cha:exper_classifier}
	Table \ref{tab:classifier_accuracies} presents the accuracies of the linear classifier, which aims to predict the syllable corresponding to the latent representation. There are a total of 9 different syllables, such that a random model would obtain an accuracy of 11\%. While the t-SNE plots in figure \ref{fig:four_tsne_plots} have demonstrated that vowels can be distinguished relatively easily, differentiating consonants poses a greater challenge. As a result, both GIM and V-GIM show mediocre test accuracies, ranging between 53\% and 54\% in the first module, and further decreasing in the second module. These accuracy values, coupled with the t-SNE plots, indicate that information regarding the consonants may no longer be adequately captured in GIM and V-GIM's latent representations. Moreover, the performance continues to decline when considering the accuracies obtained from the two second modules.
	
	\input{graphs/accuracies}
	
	We believe that these mediocre accuracies can be attributed to the limited mutual information between temporally nearby patches. GIM and V-GIM assume data that adheres to the slowly changing features assumption, which is necessary to maximise the mutual information between the latent representations of temporally nearby data patches. Consequently, abrupt changes in the patches are discarded from the latent representations. 
	However, in general, the words spoken in the dataset consist of longer durations for vowels compared to consonants. For instance, in the syllable ``ba", the phoneme ``b" is usually pronounced for a shorter duration than the ``a". This can cause a problem when the phone ``b" is not captured over multiple patches, as only the mutual information between the patches is kept. Additionally, latent representations are optimised to maximise the mutual information with the future $k$ patches. Since $k$ remains fixed over the different modules and deeper modules capture longer time windows, the latent representations of deeper modules must capture more information, while the number of channels remains relatively small, at 32.
	
				
	\subsection{Distributions}
		% distr images
		Figures \ref{fig:distr_module1_beta0035}, \ref{fig:distr_module2_beta0035}, \ref{fig:distr_module1_beta0} and \ref{fig:distr_module1_beta0} depict the distributions of activations corresponding to an individual dimension. Each sub-figure depicts the distributions for a single module. We show distributions for the first 6 dimensions. We observe that V-GIM does indeed learn to constrain the latent space to the standard normal for most dimensions. Meanwhile, in GIM's first module, we observe high and thin peaks, indicating that the same value is regularly predicted with little noise due to $\sigmafat \approx 0$. The peaks are less dominant in the second module, which may be attributed to suboptimal convergence.
		
		% TODO: mention that since v-gim is gaussian distributed, we can do our interpolation analysis?

		\input{distributions}


	\section{Generalisation study} \label{cha:generalisation_study}
		In contrast to GIM, V-GIM's latent representations are samples from a distribution, resulting in a single patch of data having multiple latent representations. In this section, we examine whether a linear classifier with little annotated training data can benefit from this representation variability introduced by V-GIM. We train multiple multi-class classifiers with the same experimental details as discussed in section \ref{cha:experim_details_vgim} but with a modification to the annotated training set. Each classifier is trained on a subset of the dataset, varying between 1 data point per class, all the way up to 128 data points. The batch size is set to the size of the subset. Training details for GIM and V-GIM remain unchanged, including the size of the training set, which does not require any labels.
		
		The test accuracies are shown in figure \ref{fig:subsets_experiment}. Overall, we observe no performance benefit from V-GIM's representation variance. Performance in GIM and V-GIM's first modules remains consistent, regardless of the subset size. Meanwhile, differences in the second module are more prominent. However, this is related to the internal batch normalisation mechanism, discussed in section \ref{cha:experiments_vgim_train_err}, resulting in faster convergence of V-GIM's modules.
		
		\begin{figure}[h] % two graphs of subsets
			\centering
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\input{graphs/subsets_module1}
				%\caption{Caption for the first graph.}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\input{graphs/subsets_module2}
				%\caption{Caption for the second graph.}
			\end{subfigure}
			\caption{Test accuracy for different subset sizes}
			\label{fig:subsets_experiment}
		\end{figure}






	


\section{V-GIM's Interpretability analysis} 
	In the following sections, we delve deeper into V-GIM's latent representations, aiming to gain an understanding of the captured information and understand underlying structures. This is achieved by employing a decoder on top of each of V-GIM's modules. By altering a representation's component values and observing the effect through the decoder, we can analyse the contained information in each individual dimension. As we argued in section \ref{cha:vgim_benefits}, this is only possible because V-GIM's latent space is optimised to be approximate to the standard normal. The decoder can then generalise to the altered representations as long as the representations are close to the origin.
	
	\subsection{Decoders for V-GIM} \label{cha:experiments_interp_analy_decoder}
		We employ two decoders, one for each module, which can be represented as follows: $D(\zt^1) = \tildex_t$ for the intermediate decoder and $D(\zt^2) = \tildex_t$ for the final decoder. This allows us to assess the information in both the final and the intermediate representations. Architecture details for the two decoders are provided in tables \ref{tab:decoder-architecure-intermidate} and \ref{tab:decoder-architecure-final}. 
		An intermediate representation $\zt^1$ with a shape of $32 \times 1$, capturing a single time step, is transformed into a speech signal $\tildex_t$ with a shape of $1 \times 448$ (or 28ms). Similarly,  the final representation $\zt^2$ is transformed into a shape of $1 \times 1024$ (or 64ms).
			
		\input{graphs/decoder_architectures}
			
		While the decoders can be optimised by minimising the Mean Squared Error (MSE) of the speech waves, this metric may not reflect well with the natural biases in human hearing. Humans perceive certain frequencies to be louder than others \citep{radkoffLossFunctionsAudio2021, liSupervisedSpeechEnhancement2020}. To account for this, we instead minimise the MSE on the mel-frequency spectrograms, which are an adaptation of linear spectrograms that emphasise frequency bins based on perceptual hearing biases \citep{shenNaturalTTSSynthesis2018}. Additionally, we employ a logarithmic transformation to account for humans' logarithmic perceptual hearing \citep{braunConsolidatedViewLoss2020}. Figure \ref{fig:example_mel_bababu} illustrates an example of a log mel-spectrogram. 
		
		The loss function we use is the following:
		$$
		\mathcal{L}_{\text{decoder}} =\frac{1}{n} \sum_{i=1}^n\left( \log (\MEL (\xt^{(i)})) -\log (\MEL (\tildex_t^{(i)} )) \right)^2
		$$
		In this equation, $\MEL (\xt^{(i)})$ represents the mel-spectrogram of the original signal $\xt^{(i)}$, computed using the following parameters: the number of FFT bins set to 4096, the length of the hop between STFT windows set to 2048, and the number of filter banks set to 128. Other parameters are kept at their default values in PyTorch's MelSpectrogram implementation \citep{paszkeAutomaticDifferentiationPyTorch2017}. Logarithms in the equation are computed in base 10.
		
		\input{graphs/sectr_example/spectr_example}
		
		
		%TODO: IF I HAVE TIME, I COULD DISPLAY SOME VISUALS OF PREDICTIONS IT HAS MADE, BUT CURRENTLY, THEY DON'T LOOK VERY GOOD.

			
	\subsection{V-GIM representation analysis through decoder}
		Decoding latent representations into their original representation as a speech wave allows us to perceive the remaining information in the representation. It is worth mentioning that the reliability of this analysis is dependent on the performance of the decoder. If it is not able to reconstruct certain features, this does not necessarily mean the information is not contained in the latent representation. 
		
		Training and validation loss curves for both decoders are shown in figure \ref{fig:decoder_loss_functions}. Overall, in both decoders, we observed that audio files could be reconstructed and the pronounced syllables were recognisable when listening to them. However, reconstructed sounds were noisier and information about the speaker's identity was unrecognisable, suggesting that this information is either no longer contained in the latent representations, or the decoders are not able to replicate it. Additionally, while vowels were easily identifiable by listening to the audio, we observed that the decoder occasionally generated incorrect consonants. For instance, decoding the latent representation for ``gu" could occasionally result in an incorrectly generated ``bu" or ``du". These observations are in line with the t-SNE plots classification performance we discussed in the previous section, further suggesting that consonant information may no longer be present in the latent representations.
		
		\input{graphs/decoder/loss_functions}
		
		By decoding the latent representations of existing speech data, we gain insight in the remaining information in the representations. However, our goal goes further: we would like to also understand the underlying structure of these representations, and attempt to understand the precise information contained in each dimension. We achieve this by starting from the zero-vector $\textbf{0}$ as latent representation, and manipulating a single dimension at a time. We can then observe the effect in the reconstructed speech signals, by looking at the it from the time and frequency domain. Figures \ref{fig:interpol_dim1}, \ref{fig:interpol_dim7} and \ref{fig:interpol_dim12} show reconstructed speech signals, obtained from the \textit{intermediate} decoder. We show the reconstructed audio waves, both in the time and frequency domain. In each figure, a single dimension is being manipulated, ranging between -2.68 and 2.68. Since the latent space is optimised to conform to the standard normal distribution, this would mean that 99\% of the data points are contained within this range \citep{bhandariStandardNormalDistribution2020}.
		
		We categorise the dimensions into one of three groups based on their contained information. Figures \ref{fig:interpol_dim1}, \ref{fig:interpol_dim7} and \ref{fig:interpol_dim12} show an example of each category. The first category contains information about frequencies for a specific frequency bin. Altering this dimension will a have strong influence on the magnitude of one or two frequency bins while other bins remain relatively unchanged. This is shown in figure \ref{fig:interpol_dim12}, where changing the value of the 12th dimension towards -2.68 causes a peek around the 100 Hz frequency bin. The peek disappears when approaching 0. Meanwhile, when approaching +2.68, the magnitude of the 150 Hz frequency bin increases. Overall, we observed this kind of behaviour for roughly half of the 32 dimensions. The majority of dimensions were sensitive in the 100 Hz and 150 Hz frequency bins, while a few were in the 175 Hz bin. These ranges are to be expected as the pitch range for men's voices is between 60 and 180 Hz \citep{rePreferencesVeryLow2012}.
		
		Dimensions that belong to the second category, not only influence the magnitude of frequencies in a specific bin, but also the neighbouring bins. In this category, modifying a dimension will cause an increase in the magnitude of a particular bin, while simultaneously influencing the surrounding frequencies. This results in a smooth envelope with a single peak which gradually decreases across the neighbouring bins. This behaviour is shown in figure \ref{fig:interpol_dim1}.
		
		The final category includes the dimensions which do not seem to make any contribution to the reconstructed signal. Changes to these cause very little change to the resulting speech wave, both in the time and frequency domain. These dimensions are either useless, or the decoder was not able to capture their contribution. An example is shown in figure \ref{fig:interpol_dim7}.
		
		Regarding the second decoder, which was trained with V-GIM's \textit{final} representations, we observe that syllables are audible, but due to disturbances, it becomes more challenging to distinguish the precise syllables and consonants. This is to be expected, as latent representations in the second module capture speech sequences of a longer time frame (64ms instead of 28ms) while remaining the same size at 32 dimensions.
		
		In the final representations, we observed more dimensions of the second category, containing the information of an entire neighbourhood of frequency bins, influencing frequencies ranging between 80 to 300 Hz. Additionally, we observed peaks in the frequency domain with much stronger magnitudes, compared to those in the intermediate representations. An example is shown in figure \ref{fig:interpol_dim7_second_module}, displaying the reconstructed speech wave in the time and frequency domain for a final representation.
		
		\input{graphs/interpolation_graphs}
	
	\subsection{Vowel experiment}
		To gain better insight in which dimensions incorporate vowel information, we train a linear classifier to predict whether the vowel corresponding to a syllable's latent representation is an ``a", ``i" or  ``u"\footnote{The phoneme sounds ``a", ``i" and ``u" are represented as ``a", ``i" and ``$\mho$" in the international phonetic alphabet (IPA) \citep{teachwonderfulLetExploreInternational2021}}. Implementation details are the same as the linear classifier in section \ref{cha:experim_details_vgim}, but with only the three classes and without a bias term. The classifier is trained on the latent representations from V-GIM's first module. Additionally, representations are max pooled over the time domain, resulting in a single 32 dimensional feature vector for each syllable. After training, a Softmax function is appended to normalise predictions into probability values. After convergence, the classifier obtained a test accuracy of 85.38\%.
		
		Figure \ref{fig:vowelclassifierweights} displays the weights of the linear classifier. The weights are scaled between -1 and 1. Each row corresponds to the weights corresponding to an individual prediction class. Equivalently, when considering a graph representation for the classifier, the weights on the edges corresponding to a particular output node, are the values in a single row. Consequently, values close to 1 or -1 indicate a high (positive or negative) correlation between the target class and the corresponding dimension. 
		
		Interestingly, most dimensions appear to contribute very little to the predicted outcome, indicating that these dimensions are either useless, or the classifier did not find value in their contained information. Consequently, most vowel information can be derived with relatively high accuracy from merely a small subset of dimensions. 
		
		In addition, we observe that certain dimensions contain information for a single vowel, while others contain information for two. Values in the first dimension for instance, have a strong correlation with the vowel ``a", while no significant correlation with ``i" or ``u" appears to be visible. Meanwhile, values in dimension 17 contain information for both ``a" and ``i", depending on whether the values approach 1 or -1.
		
		Figure \ref{fig:vowelclassifierweightsheatmap} displays the decision boundary that was learned by the consonant classifier. We observe relatively thin ``grey zones" where the probabilities are not particularly decisive for a single consonant. For the majority of data points in the latent space, the classifier thus makes predictions with relatively high certainty. In addition, the area corresponding to the prediction ``u" is smaller than the other areas. This could potentially be explained as the sounds ``a" and ``i" can easily be distinguished, while the sound for ``u" is somewhere in between, and thus harder to distinguish.
		
		
		
		
		
	\begin{figure}
		\centering
		\includegraphics[width=1\linewidth, trim={0 10cm 0 10cm}, clip]{graphs/vowel_classifier_weights}
		\caption{
			Visualisation of weights in the linear vowel classifier. Latent representations from V-GIM's first module are used. The classifier is trained without bias terms.
		}
		\label{fig:vowelclassifierweights}
	\end{figure}
	
	
	\begin{figure}
		\centering
		\includegraphics[width=0.7\linewidth, clip]{graphs/vowel_classifier_weights_heatmap}
		\caption{Decision boundary in V-GIM's latent space where dimension 1 and 17 are being perturbed while the other dimensions remain fixed at 0.}
		\label{fig:vowelclassifierweightsheatmap}
	\end{figure}
	
	
	
	
		
		
	%			
	%			
	%			To keep:
	%			%D:\thesis_logs\logs\good models\interpolate_module_1\plots_interpolate_fourier\dim=11
	%			% either 100 or 150 hz.
	%			
	%			%dim=0: strong peak at 100 and also environment
	%			
	%			%dim 6
	%			
	%			
	%			dim=0: strong peak at 100 and also environment
	%			dim1 same
	%			dim2: starts high peak at 100, but other end of spectrum is 150 strong peak
	%			dim3: 0 to 3: at 3 high peaks all around 100 (0 - 300 hz)
	%			so can see that some are sensitive for merely single freq, others for multiple freqs!
	%			dim12 too, dim10, 25
	%			
	%			dim4: same as dim2
	%			dim7: same as dim2 (either peak at 100hz or 150hz)
	%			dim11: same: either peak at 100hz or 150hz.
	%			dim14, dim15, 16, 17, 18, 19, 22, 26, 31
	%			
	%			dim6: does nothing. could be a dimension that does not contribute to anything.
	%			%	 dim8 same
	%			dim13, 20, 30
	%			
	%			dim 21: sensitive around 180-200
		

			
			
		% result:
			% Timesteps in second module captures much wider time frame and first module, so observes different content.
			%		First module: each component respoble for different frequency.
			%
			%		second module:
			%		combination of frequencies.
		
	
		%Fig \ref{fig:bagidi1-model29-true-vs-predicted} displays the reconstructed signal from the vocal sound ``ba-gi-di". The two images on the left displays the original signal, while the right two images contain the reconstructed signal.  The upper images displays the signals in time domain, the bottom images spectral domain. The reconstructed signal is an audio sample, for instance which is encoded via Greedy Infomax (up to the fourth (and final) convolution layer), this output is then given to a decoder to reconstruct the original signal.
	
		

