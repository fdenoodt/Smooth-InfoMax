\chapter{Experiments}

In this chapter we assess the encodings obtained from Variational Greedy Infomax (V-GIM) and compare results against its non-variational counterpart, GIM. We train the encoders on sequential data from the audio domain and assess the encodings' quality. The assessment is achieved by projecting the encodings into a two-dimensional space using t-SNE and observing the emergence of possible clusters. Additionally, the representations obtained from the encoders are used as input for a linear classifier, whose accuracy scores provide insights in the representations' performance.

Finally, we gather more insights in the encodings. We assess the amount of labelled data required for obtaining adequate performance in downstream tasks, by training the linear classifier on smaller subsets of the dataset. We also provide insights in the underlying structure of the encodings, by training a decoder on top of each of V-GIM's modules.


%- Experimental details encoder:
%	- Learning task: to find representations for speech data.
%	- Dataset:
%		- full data + split up
%	- Architecture
%- Results
%	- Loss functies
%	- Linear separability (t-sne)
%	
%- Generalisation (generalisation)
%		- Accuracies Subsets
%- Interpretability
%		- Encoder distributions (shows that interpolation can make sense)
%		- Decoder: loss, architecture

	






% TODO: I CALL IT Variational Greedy Infomax: V-GIM

\section{Experimental details encoder}

	
	
	\subsection{Dataset}
		The Greedy Infomax model is trained on speech data. The model takes as input a raw speech signal of a fixed length and outputs a latent representation for that signal. The dataset is split up into 729 training files and 122 test files. In each file consists of a single spoken sound consisting of three consonants and three vowels, where the consonants and vowels alternate each other. Some Examples are the sounds "gi-ga-bu" and "ba-bi-gu". All the sounds are spoken by the same person, at a constant and peaceful \textbf{todo: describe emotional aspects of speech audio}. 
		
		The following transformations are applied to the audio files. Although the original contains a sample rate of 441 Khz, the audio files are downsampled to 16 Khz, matching the sample rate used by LÃ¶we \cite{lowePuttingEndEndtoEnd2020}. This significantly reduces the size of the latent representations, and thus the required amount of VRAM during training. Additionally, two types of noise are added to the data. We apply Gaussian white noise, at different decibels ranging between zero and fifteen \textbf{TODO ...}.
		
		- also background noise from dataset. is a way to enlarge our dataset. 
		- Each audio file is cut to have length \textbf{10240}. Additionally, 
		
		
		Splitting: see appendix  \ref{appendix:split_syllables}
		
	\subsection{Architecture}
		% cnn + gru: so inputs can have variable lengths
			Training is done speech signals of fixed length, eg 8,800 samples. Notice however, that the neural network only makes use of convolutional neural networks layers and GRU's, no fully connected layers. The input dimensions can therefore be variable during inference. Only the number of channels in the latent representations should be constant, but the length can change.
			
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.7\linewidth]{architecture}
			\caption{}
			\label{fig:architecture}
		\end{figure}
	
		Figure \ref{fig:architecture} displays how via contrastive predictive coding an input speech signal is transformed into two latent vectors. The two vectors combined describe a Gaussian distribution for each feature of the latent representation. One vector corresponds to the means and the other to the standard deviations of the latent distributions. We variational autoencoders assume independent latent features, such that the covariance matrix is non negative on the diagonal and zero off the diagonal. This allows the covariance matrix to be described using a single vector. We again, make use of this (plausible incorrect?) assumption of having independent latent features.




		% Training on longer patches, classification on padded sub-patches.
		During inference, (in this context obtaining the latent representations for our input signals), depending on the length of the input signal, the length of the output latent representation will differ.
		If we wish to look at how separable latent representations are for syllables, the length can be variable. Some input sounds could be 6,600 samples, while others 8,800 samples. We therefore pad the syllables with zeroes in front and end of the signal, to obtain fixed length of equal to that of the longest syllable; 8,800 samples.
		
		Training happens on longer data samples, and every \textbf{X} epochs t-SNE visualisations are made to observe evolutional of dis-entanglement.
		

\section{Results}

	% Enkele loss curces etc
	\subsection{Results CPC}
	
	\begin{figure}[h] % distr beta = 0, module 1
		\centering
				\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\input{graphs/kld=0035_loss_0}
			\caption{Module 1: $\beta=0.0035$}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\input{graphs/kld=0035_loss_1}
			\caption{Module 2: $\beta=0.0035$}
		\end{subfigure}		
		\hfill
		
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\input{graphs/kld=0_loss_0}
			\caption{Module 1: $\beta=0$}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\input{graphs/kld=0_loss_1}
			\caption{Module 2: $\beta=0$}
		\end{subfigure}
		
		\caption{Training and validation loss}

	\end{figure}

	
	
	\subsection{Distributions}
	
	% distr images
		\input{distributions}
	
	
	% loss func only shows part of picture -> towards tsne
		Although loss is used as evaluation metric, it only shows part of the picture. The main objective for the representations is to obtain some form of "decoupled/dis-entanged" features that are more easily separable. This evaluation is done by projecting the latent representations to a 2D plane, via t-SNE. Then datapoints are coloured in depending on their the syllable that was pronounced, eg: "gi" or "ga".
		
		
	
	
	
	\subsubsection{T-sne}
	T-distributed stochastic neighbour embedding.
	
	
	Visualising data using T-sne.
	given N high-dimensional objects x1 .. xN. wants to see underlying structure of the data. eg clusters? local structures?
	
	How visiualise very high dimensional data?
	
	Introduction:
		build map where similar data points are moved close to each other and unsimilar points far away. and map in eg 2 or 3 dims. (scatter plot).
		
		
	
	
		%	\ref{fig:tsne_two_module_kld_0}: can see that second model harms performance. We believe this can be explained via the learning rate. \ref{fig:tsne_two_module_kld_0033}, there we see that second module performs better separation, indicating that the intermediate KL convergence constraint (causing the normal Gaussian distributions) also serves as a batch normalisation term, and thus resulting in faster convergence.
	%	\textbf{T-sne visualisations:}
	%	Multiple models trained, trained GIM, but single module (exact CPC architecture), one with autoregressor and once without autoregressor layer, so only CNN layers. 200 epochs, trained on split up data samples.
	
	%	
	%	4) old GIM with all modules each one layer. l1 .. 5 - cnn, l6 = gru. img shows l5 = cnn:
	%	model can more easily distinguish A's from other \textbf{klinkers}. Partly, it kinda makes sense for GIM to learn to separate \textbf{klinkers}. Since they last longer (longer duration), the loss function will more likely randomly sample a subwindow from the "aa's" than from the \textbf{medeklinker} part.
	%	
	
	
	\begin{figure}[ht] % four t-sne images
		\centering
		\begin{subfigure}{0.45\linewidth}
			\centering
			\includegraphics[width=\linewidth]{"t-sne kld=0.0033 module 1"}
			\caption{Module 1: $\beta=0.0035$}
			\label{fig:t-sne-kld33-module1}
		\end{subfigure}
		\hspace{0cm}
		\begin{subfigure}{0.45\linewidth}
			\centering
			\includegraphics[width=\linewidth]{"t-sne kld=0.0033 module 2"}
			\caption{Module 2: $\beta=0.0035$}
			\label{fig:t-sne-kld33-module2}
		\end{subfigure}
		\vspace{0cm}
		\begin{subfigure}{0.45\linewidth}
			\centering					
			\includegraphics[width=\linewidth]{"t-sne kld=0 module 1"}
			\caption{Module 1: $\beta=0$}
			\label{fig:t-sne-kld0-module-1}
		\end{subfigure}
		\hspace{0cm}
		\begin{subfigure}{0.45\linewidth}
			\centering
			\includegraphics[width=\linewidth]{"t-sne kld=0 module 2"}
			\caption{Module 2: $\beta=0.0035$}
			\label{fig:t-sne-kld0-module-2}
		\end{subfigure}
		\caption{My images}
		\label{fig:myimages}
	\end{figure}

		
		
	
		
		
	



\section{Generalisation}
	help!!
	
	\begin{figure}[h] % two graphs of subsets
		\centering
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\input{graphs/subsets_module1}
			%\caption{Caption for the first graph.}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\input{graphs/subsets_module2}
			%\caption{Caption for the second graph.}
		\end{subfigure}
		\caption{Validation accuracy for different subset sizes}
	\end{figure}






	


\section{Interpretability}

	\subsection{Decoders for Variational Greedy InfoMax}
		% investigate what info in V-GIM + argue can interpolate
		To investigate what information is contained in V-GIM's representations, we train a decoder on top of each of V-GIM's modules. Contrary to variational autoencoders, V-GIM is fully independent from the decoder and does not require one for training. Furthermore, we analyse the underlying structure the representations obtained from each module. This is achieved by altering representation's component values and observing the effects through the decoder. As we argued in the previous section, this is only possible because V-GIM's encodings is optimised to be approximate to the standard normal. As long as this is case, there are no "gaps" in the latent space around the origin. The decoder will be able to generalise to the altered representations as long as the representations are close to the origin.
		
		We train a decoder for each of V-GIM's modules. As such we can assess the information contained in the final representation, but also the representations of intermediate modules.
		
		% result:
		timesteps in second module captures much wider time frame and first module, so observes different content.
		first module: each component respoble for different frequency.
		
		second module:
		combination of frequencies.
		
		\subsubsection{Decoder architecture}
		We develop two decoders, one for each module. 
		$$
			\text{decoder}^1(\zt^1) = \xt
		$$
		$$
			\text{decoder}^2(\zt^2) = \xt
		$$
		Both decoders' architectures are symmetric to the architecture the V-GIM's encoder. Where the convolutional and max pooling layers are both replaced by transposed convolution layers.
		The architecture for 
		
		\begin{tabular}{|c|c|c|c|}
			\hline
			Layer & Filter size & Stride & Padding \\
			\hline
			TransConv1 & 3 & 1 & 1 \\
			\hline
		\end{tabular}
	
	
	
		% -----
		The decoder consists of a convolutional neural network which takes as input the encodings from V-GIM and is tasked to reconstruct the original data. The decoder is optimised to minimise the mean squared error between mel spectrograms of the original data and the reconstructed data. 
		
		We optimise the decoder with the loss function introduced in [cite] and alter it to use mel spectrograms to better capture the important speech features according to the auditory system. The loss function we use is the following:
		$$
		\mathcal{L}_{\text{decoder}} =\frac{1}{n} \sum_{i=1}^n\left( \log (MEL(y^{(i)})) -\log (MEL(\hat{y} ^{(i)} )) \right)^2
		$$
		
		
		\subsection{Decoder results}
		
		
		
	
	
	
	
	% first module:
	
%	class SimpleV2Decoder(GimDecoder):
%	def __init__(self, hidd_channels=32, out_channels=1):
%	super().__init__("Simple_v2_DECODER")
%	
%	# Encoder architecture (Simple v2)
%	kernel_sizes = [10, 8, 3]
%	strides = [4, 3, 1]
%	padding = [2, 2, 1]
%	max_unpool_k_size = 8
%	max_unpool_stride = 4
%	
%	# Decoder architecture
%	self.decoder = nn.Sequential(
%	nn.ConvTranspose1d(hidd_channels, hidd_channels,
%	kernel_sizes[2], stride=strides[2], padding=padding[2]),
%	nn.ReLU(),
%	
%	# Replaces maxpooling
%	nn.ConvTranspose1d(hidd_channels, hidd_channels, max_unpool_k_size,
%	stride=max_unpool_stride, padding=0, output_padding=0),
%	nn.ReLU(),
%	
%	nn.ConvTranspose1d(hidd_channels, hidd_channels,
%	kernel_sizes[1], stride=strides[1], padding=padding[1], output_padding=1),
%	nn.ReLU(),
%	
%	# Replaces maxpooling
%	nn.ConvTranspose1d(hidd_channels, hidd_channels, max_unpool_k_size,
%	stride=max_unpool_stride, padding=0, output_padding=3),
%	nn.ReLU(),
%	
%	nn.ConvTranspose1d(hidd_channels, out_channels,
%	kernel_sizes[0], stride=strides[0], padding=padding[0], output_padding=2),
%	)
%	
%	def forward(self, x):
%	return self.decoder(x)
	
	
	


% Second module:
%
%class SimpleV3DecoderTwoModules(GimDecoder):
%def __init__(self, hidd_channels=32, out_channels=1):
%super().__init__("Simple_v3_2Module_DECODER")
%
%kernel_sizes = [6, 6, 3]
%strides = [2, 2, 1]
%padding = [2, 2, 1]
%
%self.module2 = nn.Sequential(
%nn.ConvTranspose1d(hidd_channels, hidd_channels,
%kernel_sizes[2], stride=strides[2], padding=padding[2]),
%nn.ReLU(),
%nn.ConvTranspose1d(hidd_channels, hidd_channels,
%kernel_sizes[1], stride=strides[1], padding=padding[1], output_padding=0),
%nn.ReLU(),
%nn.ConvTranspose1d(hidd_channels, hidd_channels,
%kernel_sizes[0], stride=strides[0], padding=padding[0], output_padding=0),
%)
%self.module1 = SimpleV2Decoder(hidd_channels, out_channels)
%
%def forward(self, z):
%z = self.module2(z)
%x = self.module1(z)  # SimpleV2Decoder
%return x

	
	
		\subsubsection{Loss function}
		
 % We use five convolutional layers with strides [5, 4, 2, 2, 2], filter-sizes [10, 8, 4, 4, 4] and 512 hidden units with ReLU activations.
	



	
	
	\subsection{Decoder: predictions on test set}
	Fig \ref{fig:bagidi1-model29-true-vs-predicted} displays the reconstructed signal from the vocal sound "ba-gi-di". The two images on the left displays the original signal, while the right two images contain the reconstructed signal.  The upper images displays the signals in time domain, the bottom images spectral domain. The reconstructed signal is an audio sample, for instance which is encoded via Greedy Infomax (up to the fourth (and final) convolution layer), this output is then given to a decoder to reconstruct the original signal.
	
	%TODO: BROKEN 
	%\begin{figure}[h]
	%	\centering
	%	\includegraphics[width=0.7\linewidth]{"../../../../../../../../../GitHub/thesis-fabian-denoodt/GIM/logs/GIM_DECODER_experiment/MSE + scFFT Loss FFT=10240 Lambda=1.0000000/lr_0.0010000/GIM_L4/predictions_model=29/test/bagidi_1, model=29, True vs Predicted"}
	%	\caption{Top left: original, time domain. Bottom left: original }
	%	\label{fig:bagidi1-model29-true-vs-predicted}
	%\end{figure}



