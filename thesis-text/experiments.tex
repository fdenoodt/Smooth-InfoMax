\chapter{Experiments}

In this chapter, we evaluate the effectiveness of the latent representations obtained from Variational Greedy InfoMax (V-GIM) in comparison to its non-variational counterpart, GIM, by training both models on sequential data from the audio domain. To assess the quality of the representations, we project them into a two-dimensional space using t-SNE and analyse the emergence of potential clusters. Moreover, we employ a linear classifier that takes these representations as input, and evaluate its accuracy to gain insights into these models their performance. To further examine the efficacy of these representations, we investigate their potential for downstream task generalisation by training the linear classifier on smaller subsets of the dataset and assessing the amount of labelled data required to achieve satisfactory performance. Finally, we delve into the underlying structure of V-GIM's representations by training a decoder on top of each of V-GIM's modules, providing insights into their composition.


%- Experimental details encoder:
%	- Learning task: to find representations for speech data.
%	- Dataset:
%		- full data + split up
%	- Architecture
%- Results
%	- Loss functies
%	- Linear separability (t-sne)
%	
%- Generalisation (generalisation)
%		- Accuracies Subsets
%- Interpretability
%		- Encoder distributions (shows that interpolation can make sense)
%		- Decoder: loss, architecture

	






% TODO: I CALL IT Variational Greedy Infomax: V-GIM

\section{Experimental details V-GIM} \label{cha:experim_details_vgim}
	% VGIM + GIM, trained on Bart's dataset, 80\%, 20 test set
		GIM and V-GIM are trained on raw speech signals of fixed length sampled at 16kHz. The dataset consists of 851  audio files and is randomly shuffled and split up into 80\% training data (680 files) and 20\% test data (171 files). Each audio file consists of a single spoken sound consisting of three consonants and three vowels, where the consonants and vowels alternate each other. Some examples are the sounds "gi-ga-bu" and "ba-bi-gu".	The words consists of three syllables from the following list: ba, bi, bu, da, di, du, ga, gi and gu. All the sounds are spoken by the same person, at a constant and peaceful tone. We crop the files to a fixed length of 10240 samples, or 640 milliseconds, which is slightly longer than half a second.
		
	% In contrast GIM's architecture introduced in LÃ¶we et al.
		GIM and V-GIM are trained on the same architecture, consisting of two encoder modules $g_{enc}^1(\cdot)$ and $g_{enc}^2(\cdot)$, no autoregressor module $\gar$ is used. Modules are trained in parallel. Each module consists of solely convolution and pooling layers. Since the architecture contains no fully connected layers, the length of the audio files can therefore be variable during inference. The architecture details for the two modules are presented in tables \ref{tab:architecure-module-1} and \ref{tab:architecure-module-2}. ReLu non-linearity is applied after each convolution, except in the last layer in each module. No batch normalisation is used. The number of hidden channels produced by each convolution and pooling layer remains constant, at 32 channels. In each module, data flows synchronously from one layer to the successive layer, except in the final two layers. These run in parallel and both take the same input from the preceding layer. This architecture choice is related to the parametrisation trick we discussed in section \ref{cha:vgim_decoupled_training_for_probabil_repr}. One layer generates $\mufat$ and the other $\sigmafat$ such that $\ztm$ can be computed via the reparametrisation trick. The architecture is visualised in figure \ref{fig:architecture}. 
		An input signal of length $1 \times 10240$ is downsampled to $32 \times 52$ by the first module and to $32 \times 13$ by the second module. Each latent representation $\zt^M = \zt^2$ captures $49$ms of speech.
		% As a result of this architecture, an input signal of length $1 \times 10240$ is downsampled to $32 \times 52$ by the first module and to $32 \times 13$ by the second module. Each latent representation $\zt^M = \zt^2$ captures $49$ms of speech.

	
		% Table
		\input{graphs/vgim_architecture}

		\begin{figure}
			\centering
			\hspace*{1.5cm}
			\begin{annotatedFigure}
				{\includegraphics[width=0.8\linewidth, trim={0 2.8cm 0 2cm}, clip]{"graphs/vgim architecture"} }
				
				
				\annotatedFigureText{0.1,0.15}{black}{0.31}{$\xt$}{8}
				
				\annotatedFigureText{0.565,0.28}{black}{0.31}{$\zt^1$}{8}
				\annotatedFigureText{0.88,0.3}{black}{0.31}{$\zt^2$}{8}
				
			\end{annotatedFigure}
			\caption{Visualisation of V-GIM's architecture.}
			\label{fig:architecture}
		\end{figure}
		
		
		Importantly, the reparametrisation trick is included in our implementation of GIM's architecture as well. However, by enforcing no constraints on the latent space, which is achieved by assigning $\beta = 0$ in $\Lvnce$, we observed that $\sigmafat$ moves towards \textbf{0} such that all the randomness from sampling from a Gaussian distribution is removed. As such, our implementation of GIM is equivalent to GIM introduced in \cite{lowePuttingEndEndtoEnd2020} but with an altered architecture.
		
	% - ADAM optimiser, decaying learning rate, batch size = 171 (= test data), 800 epochs
	% - 10 negative samples, predict k = 12 time steps in the future
		Both GIM and V-GIM are trained using the Adam optimiser for 800 epochs with an exponential learning rate scheduler \cite{bhargavladGuidePytorchLearning}, with initial learning rate $lr_{init} = 0.01$ and $\gamma=0.995$, such that $lr$ is updated as follows:
		$$lr_{epoch} = \gamma * lr_{epoch - 1}$$
		The batch size is set to 171, which is exactly the size of the test set. The number of patches to predict in the future $k$, is set to 12. Implementation details with regards to drawing negative samples for $\fkmblank$ remain identical to the experiments from \cite{oordRepresentationLearningContrastive2019} and \cite{lowePuttingEndEndtoEnd2020}. The regularisation importance term $\beta$ is set to 0 for GIM and 0.0035 for V-GIM, which is the largest value we could obtain without causing posterior collapse.

	%	% classifier:
	%	After convergence, both methods trained on multi class logistic regressor?	linear multi-class classifier
	
		After training V-GIM and GIM for 800 epochs, we train a linear multi-class classifier for 100 epochs on their respective latent representations for every module, evaluated using Cross-Entropy \cite{hoRealWorldWeightCrossEntropyLoss2020}. We set $lr=0.001$ and use the Adam optimiser. The classifier is tasked to predict the syllable corresponding to its latent representation. Details on the algorithm used to split up audio files by individual syllables is provided in appendix \ref{appendix:split_syllables}. Representations are average pooled to a single time step, the number of channels remains unchanged at 32. The classifier thus consists of a single fully connected layer with 32 input nodes and 9 output nodes, conforming the number of input features and number of classes respectively. We reshuffle the dataset and use 80\% for training and 20\% for testing.

		

\section{Results}

	\subsection{Results GIM and V-GIM}	
	\subsubsection{Training error}
	% 4 Loss functions (2 modules, gim+vgim)
		
		
		Figure \ref{fig:four_losses} displays the loss curves corresponding to GIM ($\beta=0$) and V-GIM ($\beta=0.0035)$ for both modules. In the first module, the test loss for GIM is lower compared to V-GIM. This is as expected because V-GIM includes an additional regularisation term in its loss function. In the second module, both GIM and V-GIM have loss curves that converge to lower values than their previous module. This suggests that the task becomes easier for the subsequent modules, indicating that the preceding module does indeed make simplified representations for the speech data.
		
		Interestingly, in the second module, V-GIM has a lower loss compared to GIM, even though V-GIM includes the regularization term in its loss function. We argue that this is related to internal covariate shift, which we discussed in section \ref{cha:vgim_batch_norm}. The large learning rate from the previous module leads to a significant "drift" in the distribution of activations during training, making it difficult for GIM's successive modules to keep up with these changes. V-GIM is less susceptible to this issue due to the internal batch-normalisation mechanism that is built into the regularisation term. Consequently, V-GIM's modules can train with larger learning rates without affecting the performance of successive modules, resulting in faster convergence.
		
		\input{graphs/gim_vgim_losses}
		  



		
	\subsubsection{T-SNE} % TODO: moet single syllable uitleggen met opsplitsing
	We project the latent representations obtained from each module to a two-dimensional plane using t-SNE \cite{maatenVisualizingDataUsing2008}. This enables us to observe potential clusters, as similar data points are mapped close together, while dissimilar points remain far away. Similar to the classifier discussed in section \ref{cha:experim_details_vgim}, t-SNE is trained on flattened representations that are split into individual syllables, without performing any pooling. We run t-SNE with random initialisation, perplexity of 30 and a learning rate of 200 for 1000 iterations.
	
	The graphs for GIM and V-GIM are displayed in figure \ref{fig:four_tsne_plots}. T-SNE was not provided any information about the class labels. Syllables containing the vowel "a" are represented with a red tint, "u" with green, and "i" with blue. We observe similar results in the first module of both GIM and V-GIM. T-SNE identifies two clusters, with one cluster corresponding to syllables containing an "a" and the other cluster to containing "u" or "i". Within the "u/i" cluster, there is still a grouping of "u" and "i" data points. However, distinguishing between "b", "d" or "g" is more challenging as data points within the clusters are entangled.
	
	In the second module, we observe more significant differences between GIM and V-GIM. In V-GIM's second module, there is clearer separation between the "i" and "u" syllables, indicating that the second module contributes to improved latent representations. However, distinguishing between the pronounced consonant remains difficult. On the other hand, GIM's second module does not appear to have converged well, as t-SNE struggles to separate the representations into meaningful clusters. Consequently, syllables are mixed together without much structure. This further emphasises how GIM is affected by internal covariate shift, while V-GIM is not.
	
	\input{graphs/four_tsne_plots}
	
	%TODO: DIT KAN IN CONCLUSIE MIS HERGEBRUIKT WORDEN
	%\ref{fig:tsne_two_module_kld_0}: can see that second model harms performance. We believe this can be explained via the learning rate. \ref{fig:tsne_two_module_kld_0033}, there we see that second module performs better separation, indicating that the intermediate KL convergence constraint (causing the normal Gaussian distributions) also serves as a batch normalisation term, and thus resulting in faster convergence.
		
	
	\subsubsection{Classifier}
	Table \ref{tab:classifier_accuracies} shows accuracies of the classifier. Results are in line with performance on t-SNE, klinkers are easily distuingished, but medeklinkers not so much.

		\begin{table}[htbp]
			\centering
			\begin{tabular}{|l|c|c|}
				\hline
				\textbf{Method} & \textbf{Accuracy Module 1} & \textbf{Accuracy Module 2} \\ \hline
				GIM         	& 0.5                   & 0.5              \\ \hline
				V-GIM         	& 0.5                   & 0.5              \\ \hline
			\end{tabular}
			\caption{Accuracies on test data}
			\label{tab:classifier_accuracies}
		\end{table}
		

		
		
		
	\subsubsection{Distributions}
		% distr images
		Figure X, Y, Z depict the distributions of latent distributions corresponding to an individual dimension. Each figure depicts the distributions for a single module. We show distributions for the first 6 dimensions. We observe that V-GIM does indeed learn to to make distributions similar to a Gaussian, meanwhile in GIM we observe strong thin peaks, that the first module learned to frequently predict the same value with little noise due to $\sigmafat \approx 0$. The peaks are less dominant in the second module, which may be attributed to suboptimal convergence, which is attributed to suboptimal learning rate, as discussed in section \textbf{XX: prev paragraph}.
		\input{distributions}


	\subsection{Generalisation study}
		help!!
		
		\begin{figure}[h] % two graphs of subsets
			\centering
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\input{graphs/subsets_module1}
				%\caption{Caption for the first graph.}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\input{graphs/subsets_module2}
				%\caption{Caption for the second graph.}
			\end{subfigure}
			\caption{Validation accuracy for different subset sizes}
		\end{figure}






	


\section{V-GIM's Interpretability analysis}

	\subsection{Decoders for Variational Greedy InfoMax}
		% investigate what info in V-GIM + argue can interpolate
		To investigate what information is contained in V-GIM's representations, we train a decoder on top of each of V-GIM's modules. Contrary to variational autoencoders, V-GIM is fully independent from the decoder and does not require one for training. Furthermore, we analyse the underlying structure the representations obtained from each module. This is achieved by altering representation's component values and observing the effects through the decoder. As we argued in the previous section, this is only possible because V-GIM's encodings is optimised to be approximate to the standard normal. As long as this is case, there are no "gaps" in the latent space around the origin. The decoder will be able to generalise to the altered representations as long as the representations are close to the origin.
		
		We train a decoder for each of V-GIM's modules. As such we can assess the information contained in the final representation, but also the representations of intermediate modules.
		
		% result:
		timesteps in second module captures much wider time frame and first module, so observes different content.
		first module: each component respoble for different frequency.
		
		second module:
		combination of frequencies.
		
		\subsubsection{Decoder architecture}
		We develop two decoders, one for each module. 
		$$
			\text{decoder}^1(\zt^1) = \xt
		$$
		$$
			\text{decoder}^2(\zt^2) = \xt
		$$
		Both decoders' architectures are symmetric to the architecture the V-GIM's encoder. Where the convolutional and max pooling layers are both replaced by transposed convolution layers.
		The architecture for 
		
		\begin{tabular}{|c|c|c|c|}
			\hline
			Layer & Filter size & Stride & Padding \\
			\hline
			TransConv1 & 3 & 1 & 1 \\
			\hline
		\end{tabular}
	
	
	
		% -----
		The decoder consists of a convolutional neural network which takes as input the encodings from V-GIM and is tasked to reconstruct the original data. The decoder is optimised to minimise the mean squared error between mel spectrograms of the original data and the reconstructed data. 
		
		We optimise the decoder with the loss function introduced in [cite] and alter it to use mel spectrograms to better capture the important speech features according to the auditory system. The loss function we use is the following:
		$$
		\mathcal{L}_{\text{decoder}} =\frac{1}{n} \sum_{i=1}^n\left( \log (MEL(y^{(i)})) -\log (MEL(\hat{y} ^{(i)} )) \right)^2
		$$
		
		
		\subsection{Decoder results}
		
		
		
	
	
	
	
	% first module:
	
%	class SimpleV2Decoder(GimDecoder):
%	def __init__(self, hidd_channels=32, out_channels=1):
%	super().__init__("Simple_v2_DECODER")
%	
%	# Encoder architecture (Simple v2)
%	kernel_sizes = [10, 8, 3]
%	strides = [4, 3, 1]
%	padding = [2, 2, 1]
%	max_unpool_k_size = 8
%	max_unpool_stride = 4
%	
%	# Decoder architecture
%	self.decoder = nn.Sequential(
%	nn.ConvTranspose1d(hidd_channels, hidd_channels,
%	kernel_sizes[2], stride=strides[2], padding=padding[2]),
%	nn.ReLU(),
%	
%	# Replaces maxpooling
%	nn.ConvTranspose1d(hidd_channels, hidd_channels, max_unpool_k_size,
%	stride=max_unpool_stride, padding=0, output_padding=0),
%	nn.ReLU(),
%	
%	nn.ConvTranspose1d(hidd_channels, hidd_channels,
%	kernel_sizes[1], stride=strides[1], padding=padding[1], output_padding=1),
%	nn.ReLU(),
%	
%	# Replaces maxpooling
%	nn.ConvTranspose1d(hidd_channels, hidd_channels, max_unpool_k_size,
%	stride=max_unpool_stride, padding=0, output_padding=3),
%	nn.ReLU(),
%	
%	nn.ConvTranspose1d(hidd_channels, out_channels,
%	kernel_sizes[0], stride=strides[0], padding=padding[0], output_padding=2),
%	)
%	
%	def forward(self, x):
%	return self.decoder(x)
	
	
	


% Second module:
%
%class SimpleV3DecoderTwoModules(GimDecoder):
%def __init__(self, hidd_channels=32, out_channels=1):
%super().__init__("Simple_v3_2Module_DECODER")
%
%kernel_sizes = [6, 6, 3]
%strides = [2, 2, 1]
%padding = [2, 2, 1]
%
%self.module2 = nn.Sequential(
%nn.ConvTranspose1d(hidd_channels, hidd_channels,
%kernel_sizes[2], stride=strides[2], padding=padding[2]),
%nn.ReLU(),
%nn.ConvTranspose1d(hidd_channels, hidd_channels,
%kernel_sizes[1], stride=strides[1], padding=padding[1], output_padding=0),
%nn.ReLU(),
%nn.ConvTranspose1d(hidd_channels, hidd_channels,
%kernel_sizes[0], stride=strides[0], padding=padding[0], output_padding=0),
%)
%self.module1 = SimpleV2Decoder(hidd_channels, out_channels)
%
%def forward(self, z):
%z = self.module2(z)
%x = self.module1(z)  # SimpleV2Decoder
%return x

	
	
		\subsubsection{Loss function}
		
 % We use five convolutional layers with strides [5, 4, 2, 2, 2], filter-sizes [10, 8, 4, 4, 4] and 512 hidden units with ReLU activations.
	



	
	
	\subsection{Decoder: predictions on test set}
	Fig \ref{fig:bagidi1-model29-true-vs-predicted} displays the reconstructed signal from the vocal sound "ba-gi-di". The two images on the left displays the original signal, while the right two images contain the reconstructed signal.  The upper images displays the signals in time domain, the bottom images spectral domain. The reconstructed signal is an audio sample, for instance which is encoded via Greedy Infomax (up to the fourth (and final) convolution layer), this output is then given to a decoder to reconstruct the original signal.
	
	%TODO: BROKEN 
	%\begin{figure}[h]
	%	\centering
	%	\includegraphics[width=0.7\linewidth]{"../../../../../../../../../GitHub/thesis-fabian-denoodt/GIM/logs/GIM_DECODER_experiment/MSE + scFFT Loss FFT=10240 Lambda=1.0000000/lr_0.0010000/GIM_L4/predictions_model=29/test/bagidi_1, model=29, True vs Predicted"}
	%	\caption{Top left: original, time domain. Bottom left: original }
	%	\label{fig:bagidi1-model29-true-vs-predicted}
	%\end{figure}



