\chapter{Experiments}

In this chapter, we evaluate the effectiveness of the latent representations obtained from Variational Greedy InfoMax (V-GIM) in comparison to its non-variational counterpart, GIM, by training both models on sequential data from the audio domain. To assess the quality of the representations, we project them into a two-dimensional space using t-SNE and analyse the emergence of potential clusters. Moreover, we employ a linear classifier that takes these representations as input, and evaluate its accuracy to gain insights into these models their performance. To further examine the efficacy of these representations, we investigate their potential for downstream task generalisation by training the linear classifier on smaller subsets of the dataset and assessing the amount of labelled data required to achieve satisfactory performance. Finally, we delve into the underlying structure of V-GIM's representations by training a decoder on top of each of V-GIM's modules, providing insights into their composition.


%- Experimental details encoder:
%	- Learning task: to find representations for speech data.
%	- Dataset:
%		- full data + split up
%	- Architecture
%- Results
%	- Loss functies
%	- Linear separability (t-sne)
%	
%- Generalisation (generalisation)
%		- Accuracies Subsets
%- Interpretability
%		- Encoder distributions (shows that interpolation can make sense)
%		- Decoder: loss, architecture

	






% TODO: I CALL IT Variational Greedy Infomax: V-GIM

\section{Experimental details V-GIM} \label{cha:experim_details_vgim}
	% VGIM + GIM, trained on Bart's dataset, 80\%, 20 test set
		GIM and V-GIM are trained on raw speech signals of fixed length sampled at 16kHz. The dataset consists of 851  audio files and is randomly shuffled and split up into 80\% training data (680 files) and 20\% test data (171 files). Each audio file consists of a single spoken sound consisting of three consonants and three vowels, where the consonants and vowels alternate each other. Some examples are the sounds ``gi-ga-bu" and ``ba-bi-gu". The words consists of three syllables from the following list: ba, bi, bu, da, di, du, ga, gi and gu. All the sounds are spoken by the same person, at a constant and peaceful tone. We crop the files to a fixed length of 10240 samples, or 640 milliseconds, which is slightly longer than half a second.
		
	% In contrast GIM's architecture introduced in LÃ¶we et al.
		GIM and V-GIM are trained on the same architecture, consisting of two encoder modules $g_{enc}^1(\cdot)$ and $g_{enc}^2(\cdot)$, no autoregressor module $\gar$ is used. Modules are trained in parallel. Each module consists of solely convolution and pooling layers. Since the architecture contains no fully connected layers, the length of the audio files can therefore be variable during inference. The architecture details for the two modules are presented in tables \ref{tab:architecure-module-1} and \ref{tab:architecure-module-2}. ReLu non-linearity is applied after each convolution, except in the last layer in each module. No batch normalisation is used. The number of hidden channels produced by each convolution and pooling layer remains constant, at 32 channels. In each module, data flows synchronously from one layer to the successive layer, except in the final two layers. These run in parallel and both take the same input from the preceding layer. This architecture choice is related to the parametrisation trick we discussed in section \ref{cha:vgim_decoupled_training_for_probabil_repr}. One layer generates $\mufat$ and the other $\sigmafat$ such that $\ztm$ can be computed via the reparametrisation trick. The architecture is visualised in figure \ref{fig:architecture}. 
		An input signal of length $1 \times 10240$ is downsampled to $32 \times 52$ by the first module and to $32 \times 13$ by the second module. Due to overlap, each latent representation $\zt^M = \zt^2$ captures $64$ms of speech. %Previously I wrote: Each latent representation $\zt^M = \zt^2$ captures $49$ms of speech. WRONG. this would be the case if there was no overlap. but single timestep t corresponds with 1024 samples. or 64 milliseconds.
		
		% As a result of this architecture, an input signal of length $1 \times 10240$ is downsampled to $32 \times 52$ by the first module and to $32 \times 13$ by the second module. Each latent representation $\zt^M = \zt^2$ captures $49$ms of speech.

	
		% Table
		\input{graphs/vgim_architecture}

		\begin{figure}
			\centering
			\hspace*{1.5cm}
			\begin{annotatedFigure}
				{\includegraphics[width=0.8\linewidth, trim={0 2.8cm 0 2cm}, clip]{"graphs/vgim architecture"} }
				
				
				\annotatedFigureText{0.1,0.15}{black}{0.31}{$\xt$}{8}
				
				\annotatedFigureText{0.565,0.28}{black}{0.31}{$\zt^1$}{8}
				\annotatedFigureText{0.88,0.3}{black}{0.31}{$\zt^2$}{8}
				
			\end{annotatedFigure}
			\caption{Visualisation of V-GIM's architecture.}
			\label{fig:architecture}
		\end{figure}
		
		
		Importantly, the reparametrisation trick is included in our implementation of GIM's architecture as well. However, by enforcing no constraints on the latent space, which is achieved by assigning $\beta = 0$ in $\Lvnce$, we observed that $\sigmafat$ moves towards \textbf{0} such that all the randomness from sampling from a Gaussian distribution is removed. As such, our implementation of GIM is equivalent to GIM introduced in \cite{lowePuttingEndEndtoEnd2020} but with an altered architecture.
		
	% - ADAM optimiser, decaying learning rate, batch size = 171 (= test data), 800 epochs
	% - 10 negative samples, predict k = 12 time steps in the future
		Both GIM and V-GIM are trained using the Adam optimiser for 800 epochs with an exponential learning rate scheduler \cite{bhargavladGuidePytorchLearning}, with initial learning rate $lr_{init} = 0.01$ and $\gamma=0.995$, such that $lr$ is updated as follows:
		$$lr_{epoch} = \gamma * lr_{epoch - 1}$$
		The batch size is set to 171, which is exactly the size of the test set. The number of patches to predict in the future $k$, is set to 12. Implementation details with regards to drawing negative samples for $\fkmblank$ remain identical to the experiments from \cite{oordRepresentationLearningContrastive2019} and \cite{lowePuttingEndEndtoEnd2020}. The regularisation importance term $\beta$ is set to 0 for GIM and 0.0035 for V-GIM, which is the largest value we could obtain without causing posterior collapse.

	%	% classifier:
	%	After convergence, both methods trained on multi class logistic regressor?	linear multi-class classifier
	
		After training V-GIM and GIM for 800 epochs, we train a linear multi-class classifier for 100 epochs on their respective latent representations for every module, evaluated using Cross-Entropy \cite{hoRealWorldWeightCrossEntropyLoss2020}. We set $lr=0.001$ and use the Adam optimiser. The classifier is tasked to predict the syllable corresponding to its latent representation. Details on the algorithm used to split up audio files by individual syllables is provided in appendix \ref{appendix:split_syllables}. Representations are average pooled to a single time step, the number of channels remains unchanged at 32. The classifier thus consists of a single fully connected layer with 32 input nodes and 9 output nodes, conforming the number of input features and number of classes respectively. We reshuffle the dataset and use 80\% for training and 20\% for testing.

		

\section{Results}

	\subsection{Results GIM and V-GIM}	
	\subsubsection{Training error}
	% 4 Loss functions (2 modules, gim+vgim)
		
		
		Figure \ref{fig:four_losses} displays the loss curves corresponding to GIM ($\beta=0$) and V-GIM ($\beta=0.0035)$ for both modules. In the first module, the test loss for GIM is lower compared to V-GIM. This is as expected because V-GIM includes an additional regularisation term in its loss function. In the second module, both GIM and V-GIM have loss curves that converge to lower values than their previous module. This suggests that the task becomes easier for the subsequent modules, indicating that the preceding module does indeed make simplified representations for the speech data.
		
		Interestingly, in the second module, V-GIM has a lower loss compared to GIM, even though V-GIM includes the regularization term in its loss function. We argue that this is related to internal covariate shift, which we discussed in section \ref{cha:vgim_batch_norm}. The large learning rate from the previous module leads to a significant ``drift" in the distribution of activations during training, making it difficult for GIM's successive modules to keep up with these changes. V-GIM is less susceptible to this issue due to the internal batch-normalisation mechanism that is built into the regularisation term. Consequently, V-GIM's modules can train with larger learning rates without affecting the performance of successive modules, resulting in faster convergence.
		
		\input{graphs/gim_vgim_losses}
		  



		
	\subsubsection{T-SNE} % TODO: moet single syllable uitleggen met opsplitsing
	We project the latent representations obtained from each module to a two-dimensional plane using t-SNE \cite{maatenVisualizingDataUsing2008}. This enables us to observe potential clusters, as similar data points are mapped close together, while dissimilar points remain far away. Similar to the classifier discussed in section \ref{cha:experim_details_vgim}, t-SNE is trained on flattened representations that are split into individual syllables, without performing any pooling. We run t-SNE with random initialisation, perplexity of 30 and a learning rate of 200 for 1000 iterations.
	
	The graphs for GIM and V-GIM are displayed in figure \ref{fig:four_tsne_plots}. T-SNE was not provided any information about the class labels. Syllables containing the vowel ``a" are represented with a red tint, ``u" with green, and ``i" with blue. We observe similar results in the first module of both GIM and V-GIM. T-SNE identifies two clusters, with one cluster corresponding to syllables containing an ``a" and the other cluster to containing ``u" or ``i". Within the ``u/i" cluster, there is still a grouping of ``u" and ``i" data points. However, distinguishing between ``b", ``d" or ``g" is more challenging as data points within the clusters are entangled.
	
	In the second module, we observe more significant differences between GIM and V-GIM. In V-GIM's second module, there is clearer separation between the ``i" and ``u" syllables, indicating that the second module contributes to improved latent representations. However, distinguishing between the pronounced consonant remains difficult. On the other hand, GIM's second module does not appear to have converged well, as t-SNE struggles to separate the representations into meaningful clusters. Consequently, syllables are mixed together without much structure. This further emphasises how GIM is affected by internal covariate shift, while V-GIM is not.
	
	\input{graphs/four_tsne_plots}
	
	%TODO: DIT KAN IN CONCLUSIE MIS HERGEBRUIKT WORDEN
	%\ref{fig:tsne_two_module_kld_0}: can see that second model harms performance. We believe this can be explained via the learning rate. \ref{fig:tsne_two_module_kld_0033}, there we see that second module performs better separation, indicating that the intermediate KL convergence constraint (causing the normal Gaussian distributions) also serves as a batch normalisation term, and thus resulting in faster convergence.
		
	
	\subsubsection{Classifier}
	Table \ref{tab:classifier_accuracies} displays the accuracies of the linear classifier, which aims to predict the syllable corresponding to the latent representation. There are a total of 9 classes, such that a random model would obtain an accuracy of 11\%. While distinguishing vowels included in syllables is relatively straightforward, differentiating consonants poses a greater challenge. This suggests that GIM and V-GIM's representations may no longer encode information on consonants. Additionally, although GIM exhibits slightly better performance than V-GIM in a single module context, it is significantly outperformed when two modules are considered.

	\input{graphs/accuracies}
	
	\textbf{todo after new experiment is done}
	We believe the not overwhelming results can be attributed to two causes:
	- Learning metric assumes slowly changing features, it could be that not enough information in common between the last and initial patch. such that the mutual information is maintained.
	
	
	In general, the speaker in the dataset proncounces vowels for a longer
	As such, words such as ``ba"
	
	are pronounced for longer time window than the consonant.
	
	- Secondly, the dimensionality reduction could be a too great bottleneck such that not all the essential information can be captured.
		
		

		
		
		
	\subsubsection{Distributions}
		% distr images
		Figures \ref{fig:distr_module1_beta0035}, \ref{fig:distr_module2_beta0035}, \ref{fig:distr_module1_beta0} and \ref{fig:distr_module1_beta0} depict the distributions of activations corresponding to an individual dimension. Each sub-figure depicts the distributions for a single module. We show distributions for the first 6 dimensions. We observe that V-GIM does indeed learn to constrain the latent space to the standard normal for most dimensions. Meanwhile in GIM's first module, we observe high and thin peaks, indicating that the same value is regularly predicted with little noise due to $\sigmafat \approx 0$. The peaks are less dominant in the second module, which may be attributed to suboptimal convergence.
		
		% TODO: mention that since v-gim is gaussian distributed, we can do our interpolation analysis?

		\input{distributions}


	\subsection{Generalisation study}
		In contrast to GIM, V-GIM's latent representations are samples from a distribution, resulting in a single patch of data to have multiple latent representations. In this section we examine whether a linear classifier with little annotated training data can benefit from this representation variability introduced by V-GIM. We train multiple multi-class classifiers with the same experimental details as discussed in section \ref{cha:experim_details_vgim} but with a modification to the annotated training set. Each classifier is trained on a subset of the dataset, varying between 1 data point per class, all the way up to 128 data points per class. Training details for GIM and V-GIM remain unchanged, including the training set.
		
		The results are shown in figure \ref{fig:subsets_experiment}.
		% todo: explain results
		In general linear classifier does not gain a significant benefit from V-GIM's representation variance. Only in the most extreme case of having only a single data point per class V-GIM does better.
		
		
		\begin{figure}[h] % two graphs of subsets
			\centering
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\input{graphs/subsets_module1}
				%\caption{Caption for the first graph.}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\input{graphs/subsets_module2}
				%\caption{Caption for the second graph.}
			\end{subfigure}
			\caption{Validation accuracy for different subset sizes}
			\label{fig:subsets_experiment}
		\end{figure}






	


\section{V-GIM's Interpretability analysis}

	\subsection{Decoders for Variational Greedy InfoMax}
	% intro		
		% investigate what info in V-GIM + argue can interpolate
		To investigate what information is contained in V-GIM's representations, we train a decoder on top of each of V-GIM's modules. Contrary to variational autoencoders, V-GIM is fully independent from the decoder and does not require one for training. Furthermore, we analyse the underlying structure the representations obtained from each module. This is achieved by altering representation's component values and observing the effects through the decoder. As we argued in section \ref{cha:vgim_benefits}, this is only possible because V-GIM's latent space is optimised to be approximate to the standard normal. The decoder can then generalise to the altered representations as long as the representations are close to the origin.
		
		We employ two decoders in V-GIM, one for each module, which can be represented as follows: $D(\zt^1) = \tildex_t$ for the intermediate decoder and $D(\zt^2) = \tildex_t$ for the final decoder. This allows us to assess the information in both the final and the intermediate representations. Architectures details for the two decoders are provided in tables \ref{tab:decoder-architecure-intermidate} and \ref{tab:decoder-architecure-final}. 
		An intermediate representation $\zt^1$ with a shape of $32 \times 1$, capturing a single time step, is transformed into a speech signal $\tildex_t$ with a shape of $1 \times 448$ (or 28ms). Similarly,  the final representation $\zt^2$ is transformed into a shape of $1 \times 1024$ (or 64ms).
			
		\input{graphs/decoder_architectures}
			
		While the decoders can be optimised by directly minimising the Mean Squared Error (MSE) of the speech waves, this metric may not reflect well with the natural biases in human hearing. Humans perceive certain frequencies to be louder than others \cite{radkoffLossFunctionsAudio2021, liSupervisedSpeechEnhancement2020}. To account for this, we instead minimise the MSE on the mel-frequency spectrograms, which are an adaptation of linear spectrogram that emphasise frequency bins based on perceptual hearing biases \cite{shenNaturalTTSSynthesis2018}. Additionally, we employ a logarithmic transformation to account for humans' logarithmic perceptual hearing \cite{braunConsolidatedViewLoss2020}. Figure \ref{fig:example_mel_bababu} illustrates an example of a log mel-spectrogram. 
		
		The loss function we use is the following:
		$$
		\mathcal{L}_{\text{decoder}} =\frac{1}{n} \sum_{i=1}^n\left( \log (\MEL (\xt^{(i)})) -\log (\MEL (\tildex_t^{(i)} )) \right)^2
		$$
		In this equation, $\MEL (\xt^{(i)})$ represents the mel-spectrogram of the original signal $\xt^{(i)}$, computed using the following parameters: the number of FFT bins set to 4096, the length of the hop between STFT windows set to 2048, and the number of filter banks set to 128. Other parameters are kept at their default values in PyTorch's MelSpectrogram implementation \cite{paszkeAutomaticDifferentiationPyTorch2017}. Logarithms in the equation are computed in base 10.
		
		\input{graphs/sectr_example/spectr_example}
		
		

		
					
			% result:
			%		timesteps in second module captures much wider time frame and first module, so observes different content.
			%		first module: each component respoble for different frequency.
			%		
			%		second module:
			%		combination of frequencies.
			
		


	
	
	\subsection{Decoder: predictions on test set}
	\begin{figure}[h]
		\centering
		\begin{minipage}[b]{0.45\linewidth}
			\centering
			\input{graphs/decoder/loss_intermediate}
			\label{fig:loss-intermediate}
		\end{minipage}
		\hfill
		\begin{minipage}[b]{0.45\linewidth}
			\centering
			\input{graphs/decoder/loss_final}
			\label{fig:loss-final}
		\end{minipage}
		\caption{Training and validation loss for the two decoders, trained with V-GIM's representations.}
		\label{fig:example_mel_bababu}
	\end{figure}
		
	
	
		Fig \ref{fig:bagidi1-model29-true-vs-predicted} displays the reconstructed signal from the vocal sound ``ba-gi-di". The two images on the left displays the original signal, while the right two images contain the reconstructed signal.  The upper images displays the signals in time domain, the bottom images spectral domain. The reconstructed signal is an audio sample, for instance which is encoded via Greedy Infomax (up to the fourth (and final) convolution layer), this output is then given to a decoder to reconstruct the original signal.
	
		%TODO: BROKEN 
		%\begin{figure}[h]
		%	\centering
		%	\includegraphics[width=0.7\linewidth]{"../../../../../../../../../GitHub/thesis-fabian-denoodt/GIM/logs/GIM_DECODER_experiment/MSE + scFFT Loss FFT=10240 Lambda=1.0000000/lr_0.0010000/GIM_L4/predictions_model=29/test/bagidi_1, model=29, True vs Predicted"}
		%	\caption{Top left: original, time domain. Bottom left: original }
		%	\label{fig:bagidi1-model29-true-vs-predicted}
		%\end{figure}



