\chapter{Discussion and future work}
% scribbr:
% Step 1: Summarize your key findings
% Step 2: Give your interpretations (on results)
% Step 3: Discuss the implications
	% link met literature, bv kl divergence geeft veel useless dimensions (is same as literature)
% Step 4: Acknowledge the limitations
% Step 5: Share your recommendations
	% = future work, eventueel gericht op die limitaties.

	
In this thesis we introduced V-GIM, providing the necessary tools to analyse the representations learned from optimising the InfoNCE objective.
% why interpretable
V-GIM's interpretability is achieved by defining latent representations as samples from Gaussian distributions which are optimised to be similar to the standard normal. This way, the dataset will cover the entire latent space around the origin, such that sampling a random point from this space is likely to result in a data point that is similar to the data. 

%By encoding each data point to its latent representation and creating a histogram for of the representations per dimension, we observed that V-GIM does indeed enforce each dimension to be standard normal, indicating that implications with respect to interpretability using the decoder are indeed applicable. 

In this section we discuss our interpretations on V-GIM's results, discuss its limitations and make recommendations for plausible future work.

\section{Interpretability}
	
	% no b, wel a
	We analysed V-GIM's representations using t-SNE plots, linear classifiers, and decoders which were trained on top of V-GIM's representations. Interestingly, the representations seemed to contain vowel information while consonant information was lacking. This was observed in both the t-SNE plots and the linear classifier results. We suggest that this is related to the duration of the phonemes. The InfoNCE objective maximises the mutual information between temporally nearby patches, assuming presence of common information in nearby data. However, the vowels in the dataset are typically pronounced for longer duration than the consonants, and thus, representations where a consonant is being pronounced have fewer information in common with nearby representations as in those representations the consonant may not be pronounced any more. These findings are in line with \cite{lowePuttingEndEndtoEnd2020} who suggest that optimisation using the InfoNCE objective favours global information rather than local. 
	
		
	The decoder provided us with information on the contained speech information that remained in the representations. While the pronounced sounds were recognisable, the decoder occasionally generated incorrect consonants, for instance by producing the sound ``ba" when the original sound is ``ga". This is further evidence indicating that representations no longer contain as much consonant information. Additionally the pronounced sounds were recognisable but sounded more disturbed and recognising the speaker's identity became almost impossible. This could be an indication that this information is no longer contained in the representations.
	

	The interpolation study provided insights in the contained information of each dimension. We discovered three categories: dimensions with information of one or two frequency bins, dimensions with information spanning an entire range of frequency bins, and dimensions with no information at all, we call these useless dimensions. Most dimensions were active around the 100Hz to 200Hz frequency range, which is similar to the pitch of a male voice.

	Interestingly, the vowel information was included in only a small subset of dimensions. This was demonstrated in the vowel experiment, in which a classifier was trained to predict the vowel corresponding to a representation from V-GIM's first module. The classifier obtained relatively high accuracies by only looking at a few dimensions, indicating that the other dimesions did not contribute to the classification.
	 This demonstrates that the size of representations could be reduced even further by removing these dimensions.
	
	% The other dimensions were thus either useless, or carried different information. 
	
	
	In the histogram plots, we obseved 
	
	% why useless dimensions appear
	We assume V-GIM's useless dimensions can be partly attributed to KL divergence in the $\Lvnce$ loss function. 
	V-GIM appears to be stuck in a local minimum, where it ends up exploiting the $\Lvnce$ in a way we would not expect. In particular, certain dimensions will carry actual information, contributing to a good InfoNCE bound, while other dimensions that do not carry any information but enforce the standard normal space, thus optimising the KL divergence. This is further shown in the distribution graphs as certain graphs are almost identical to the standard normal distribution, while others have larger differences.
	
	%some dimensions were very nicely Gaussian shaped while others were not, indicating that optimisation function could potentially maintain useless dimensions to keep up good cost function, while containing all information. So V-GIM exploits loss function, not necessarly learning as we expect.
	
	This problem of useless dimensions is known to VAEs as well, and can for instance be mitigated by choosing alternative prior or posterior \citep{tomczakVAEVampPrior2018}. Further work could explore alternative priors introduced to VAEs and investigate their performance in V-GIM. Alternatively, the problem of useless dimensions could potentially be mitigated via a regularisation term applied to the weights of the similarity score function $\fkm$ or discriminator, enforcing similar weights between different dimensions. As such each dimension would be encouraged to contribute an equal amount to the contained information. 
	
	With regards to representation disentanglement, \cite{burgessUnderstandingDisentanglingBeta2018} argue that choosing a standard normal prior in VAEs encourages disentangled representations. As a consequence of our approach these ideas are also applicable to V-GIM and should be further verified in future work.
	

	% V-GIM enables interpretability of internal representations as well, potentially enabling to understand the network	 
	further work, could take to more extremes a single module per layer such that each layer can be analyses and then potentially also the weights.


	
	
	% disclaimer: decoder only works on V-GIM, not GIM.
	%	It is noteworthy that insights obtained from decoder only viable because of V-GIM's regularisation term, and not in regularisation-free MI maximisation techniques such as CPC and V-GIM.
	%	
	
	%Additionally, the addition of an optional decoder provided benefits over encoder-decoder architectures, as the decoder was not required for training, providing more available memory for more complex architectures.
	
	%	Additionally, V-GIM's module architecture enables the observation of interpretable activations in between the network as well. Further work could investigate these internal representations further, so not only do we have the means to understand the final representations, as X, Y, we even have the means to understand internal, potentially giving even better insights in how neural networks operate. While other representation learning techniques, x,y,z claim interpretable representations, V-GIM's representations are interpretable, but additionally provide insights in the internal mechanism of the network as well.
	
	%	Note that:
	%		Interpretability analysis gives insight in information that is available, but making claims on what information is not included is harder, as could be that decoder didn't see it. interpretability analysis gives insight in information that is available, but making claims on what information is not included is harder, as could be that decoder didn't see it. 
		
	%	 	- some dimensions were very nicely Gaussian shaped while others were not, indicating that optimisation function could potentially maintain useless dimensions to keep up good cost function, while containing all information. So V-GIM exploits loss function, not necessarly learning as we expect.
	%	 	- test alternative priors van VAE
	%		 - test disentanglement
		




		
\section{Performance benefits and representation variability}
		% beter in discussie van results?
		\cite{oordRepresentationLearningContrastive2019} and \cite{lowePuttingEndEndtoEnd2020} observed better performance in their papers for a similar speech recognition problem. We believe this is related to the size of individual representations, as they have 512 rather than 32 as in our work. Additionally, our representations capture longer time sequences and have a smaller resulting in potentially less mutual information between temporally nearby patches. Due to computational constraints, and since our primary concern was to analyse these representations, we maintained our own architecture where representations contain longer time frames.


		- performance is worse in second module, further attributed to less mutual information.

		- didn't observe a benefit from variation variance, even for the smallest subset sizes.
		- We hypothesised representation variance, resulting in more data points could improve the decision boundary, encouraging the boundary to be similar as to SVM, where distance is maximised for each point.
		- Taking inspiration from SVM's decision boundary, we investigated whether V-GIM's representation could offer a similar approach for alternative downstream tasks, is a potential 
			- encourage a better boundary, similar to SVM's \citep{hearstSupportVectorMachines1998, nobleWhatSupportVector2006}, maximising the margin between classes.
			 --> didn't seem the case.
		- explanation on why should be further investigated. One hypothesis is related to SVM's ...
		
			
\section{batch normalisation}
		- sindy didn't have issues of batch norm, but believe this is because each module consisted of a single layer, ours contain a number of layers. potentially: outputs from first module change too fast for second module to catch up.
	
\section{memory}
		- while V-GIM's greedy approach allows for distributed learning, enabling training on resource constraint devices. each module has a memory overhead due to the similarity score fkm, causing the combined memory requirements to be higher.
		- while GIM argues to resolve memory constraints, not entirely true. In fact we even countered the opposite as containing multiple neural networks, each with their own personal loss function (the loss function is based on fk which contains parameters that must be learned), and thus for early layers where the sequence is still long, a lot of memory is required. We went for a compromise on GIM by splitting up the architecture in merely two modules, significantly reducing the memory constraints.
	








%-- 
%decaying learing rate:
%we train using decaying lr, because models must first learn distributions and goes too slow if lr is too small.
%and a learning rate scheduler
%ExponentialLR
%decay rate 0.995
%
%---








%---
%The second module in GIM clearly doesn't have as much effect. This can be explained because there may not be as much common information anymore between the patches. There may be a source that says that cpc learns low level features, but the second module is supposed to learn more high level features, which cpc may have trouble with?
%---
%
%Future work:
%- Related work in VAE shows that gradually increasing regularisation term, results in better disentengledment, while avoiding posterior collapse. could have a kldweight scheduler.
%
%- not constrained solely to InfoNCE loss, the GIM architecture could work for other losses too that allow for greedy optimisation.
%
%
%- I didn't add an autoregressor as i didn't find a performance benefit. Potentially, with larger architecture could further improve performance.
%
%
%
%----
%Towards production setting:
%encodings are thus optimised to be close the standard normal. When in a production environment and new data is given, could in fact have an idea of how well generalisation to the production data: eg via anomaly detection if encodings are too far away from center. 
%= gives automated way of verifying generalisation.
%
%can then maybe see to which data that doesn't generalise well via outliers.
%
%----
%
%
%future work:
%- disentanglement should do more investations
%
%
%---
%GIM: Modular training
%could incrementally increase numb of modules and observe performance increase for downstream tasks.
%based on this, could find smallest gim architecture depth which satisfies required accuracies.
%
%----
%interpretability:
%most dimensions sensitive around 75 to 150 hz. this is as expected as the adult man speaks around 80 to 180 hz.
%
%---
%interpretability is only as good as the decoder. if a shitty decoder doesn't construct well, doesn't necessarily give correct conclusions about V-GIM.
%
%
%---
%Future work: alternatieve prior (zie related work.)
%
%
%
%
%
%
%\begin{itemize}
%	\item Explainability of latents is dependent on the performance of the decoder.
%	\item Intermediate loss function with kld resulted in similar behaviour as batch normalisation. Resulting in faster convergence than without kld.
%	\item We observed no quality loss in the learned representations. Data was equally easily separable.
%\end{itemize}