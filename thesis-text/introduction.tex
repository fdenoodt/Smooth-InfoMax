\chapter{Introduction}
%https://www.scribbr.com/dissertation/introduction-structure/

\setcounter{page}{1} % Set the page counter to 3
\pagenumbering{arabic}



Black-box models have shown incredible performance in recent years. Notable breakthroughs include AlphaGo defeating the world champion Go player in 2016 \citep{fuAlphaGoMonteCarlo2016}, and ChatGPT, which can assist with coding, reason through problems and generate well-written documents.

These models are typically trained using deep neural networks in an end-to-end fashion. Despite their impressive success, their lack of interpretability poses a significant challenge. The internal workings of these models are often not well understood, limiting their applicability in high-stakes decision-making scenarios, such as medical diagnosis, law or defence. A simple binary answer does not suffice; we need to ensure these models make decisions for the right reasons \citep{alammarExplainableAIGuide2021}. Additionally, a comprehensive understanding of these models is essential if humans wish to learn from them, for instance by applying strategies similar to those learned by AlphaGo.

While models that are interpretable by design do exist (e.g., logistic regression, linear classifiers and decision trees), they are limited by their simplicity and are not always able to model the complex problems in our world. Therefore, for certain problems, more complex models are required which results in reduced interpretability.

% post hoc methods
To gain a better understanding of these less interpretable models, various post-hoc approaches have been explored. These approaches aim to find explanations for models that are not interpretable by design, for instance through techniques such as visual explanations, text explanations and feature attribution explanations \citep{barredoarrietaExplainableArtificialIntelligence2020a}.

Some feature attribution approaches include LIME, SHAP, and gradient-based methods, which compute a relevance or contribution score for each input feature with respect to the output prediction \citep{simonyanDeepConvolutionalNetworks2014, ribeiroWhyShouldTrust2016, lundbergUnifiedApproachInterpreting2017, molnarInterpretableMachineLearning2022}. These approaches are frequently used for image classification via heatmaps, highlighting the important pixels that had the largest contribution to the decision.

While these approaches provide some insights into what the important key factors are that led to an individual prediction, they do not say much about the internal works of the model. Particularly, in artificial neural networks (ANN), it is known that specific neurons are sensitive to particular concepts. \citeauthor{zeilerVisualizingUnderstandingConvolutional2013} show that the initial layers in ConvNets trained for image classification, learn to recognise abstract concepts such as lines and edges, while deeper layers learn more semantically meaningful concepts such as the shape of a nose or ear, without being explicitly told to do so \citep{zeilerVisualizingUnderstandingConvolutional2013}.


Continuing in the line of post-hoc approaches, multiple explainability methods have been investigated that analyse the internal neurons, resulting in an improved understanding of the underlying mechanisms of these ANNs. Notable contributions aim to find the image that maximally activates a specific neuron \citep{simonyanDeepConvolutionalNetworks2014} (or, equivalently, a channel in a ConvNet). A second approach aims to, highlight the regions of an image that a particular neuron is sensitive to \citep{zeilerVisualizingUnderstandingConvolutional2013}.

These post-hoc approaches provide some level of explainability to the predictions and the interpretability of the black-box models. However, many approaches focus on images, as their explanations are more easily interpretable. Conversely, when it comes to input features that are less visually interpretable, such as raw speech signals, utilising these explainability methods may be impractical for such models \citep{krugAnalyzingVisualizingDeep2021}.

In addition, \cite{bauNetworkDissectionQuantifying2017} argue that the internal semantic concepts learned by these neurons are highly entangled throughout the network. This makes interpretation of neurons particularly difficult, as multiple neurons may work as a whole and together be sensitive for a semantic concept such as a nose \citep{molnarInterpretableMachineLearning2022}, meaning that there may not always be an individual neuron that is responsible for a specific concept. Additionally, these neurons may even be spread out over multiple layers.  Optimally, we require fully disentangled ANNs where each single neuron is sensitive for a unique feature. In addition, \citeauthor{bauNetworkDissectionQuantifying2017} found that many neurons could not be linked to a specific concept \citep{molnarInterpretableMachineLearning2022}.

For these reasons it is likely impossible to fully understand these black-box models. Instead, we pose the question of whether we can incorporate interpretability requirements into the design of the network. Enforcing additional constraints to the loss function, such as disentanglement, could potentially improve the interpretability of these ANNs, thereby improving the effectiveness of these post-hoc explanations while maintaining good performance.


% towards representation learning
Additionally, these concerns regarding interpretability are especially relevant in representation learning. As neurons in the network learn semantic concepts of the data, their activations at a particular layer can potentially serve as an alternative representation for the data. This notion of learning new representations for data is known as representation learning \citep{le-khacContrastiveRepresentationLearning2020}. These representations can then serve as the inputs for downstream tasks such as a classification or regression problem. However, to ensure that these downstream tasks make meaningful predictions, we must ensure that the representations they are trained on contain meaningful information. Therefore, a good understanding of the representations is needed.


One approach for representation learning is the Variational Autoencoder (VAE), which indirectly encourages interpretable representations imposing additional constraints to the space of plausible representations, which is commonly referred to as the \textit{latent space} \citep{kingmaAutoEncodingVariationalBayes2022}. Additionally, recent work has shown that the latent space constraint in VAEs encourages disentangled representations, further improving upon interpretability \citep{burgessUnderstandingDisentanglingBeta2018, sikkaCloserLookDisentangling2019, higginsBetaVAELearningBasic2022}.

Contrastive methods are another representation learning framework that have seen interest in recent years, learning representations for data by discriminating them against noise \citep{chenSimpleFrameworkContrastive2020}. In particular, we focus on the information maximisation introduced by \cite{oordRepresentationLearningContrastive2019} in Contrastive Predictive Coding (CPC), which learns representations for sequential data by predicting the representations of future observations given those from the past \cite{henaffDataEfficientImageRecognition2020}. CPC has demonstrated remarkable success in a wide range of domains and is currently considered the state-of-the-art method for representation learning from sequential data without labels \citep{stackeEvaluationContrastivePredictive2020, dehaanContrastivePredictiveCoding2021, luSemiSupervisedHistologyClassification2019, bhatiSegmentalContrastivePredictive2021, deldariTimeSeriesChange2021, henaffDataEfficientImageRecognition2020}. However, CPC does not directly include an interpretability aspect to its representations, and the underlying representations obtained by optimising the InfoNCE objective in CPC have not been thoroughly studied.



To address these challenges, we introduce Variational Greedy InfoMax (V-GIM), a representation learning framework that learns interpretable representations by enforcing constraints on the latent space. V-GIM splits the architecture into modules, each trained greedily using a novel loss function that promotes interpretable representations. We introduce a post-hoc explainability approach via a decoder which utilises the constrained space to ensure meaningful decodings. This allows for the analysis of neurons at different depths in the architecture, enhancing interpretability for both the final and internal representations. In this thesis, we examine V-GIM's performance and analyse the interpretability of its learned representations, with a focus on raw speech waves.



Our contributions are the following:
\begin{itemize}
	\item V-GIM enables better understanding of the learned representations, enabling observation of the information in individual neurons, both in the final and internal representations. % This provides better interpretability compared to other interpretable techniques such as InfoGAN, ...
	\item V-GIM offers computational benefits, including internal batch normalisation, outperforming its non-variational counterpart GIM in certain cases. V-GIM's stochastic approach results in more variability, potentially serving as a data augmentation tool for downstream tasks with little labelled data.
	\item We use a decoder to gain insights into V-GIM's learned representations, making suggestions on which data is incorporated in which neuron and which information may no longer be present in the representations.
\end{itemize}




The structure of this thesis is as follows: in chapter \ref{cha:2}, we provide an overview of the necessary background information needed to fully understand our work, which is described in chapter \ref{cha:3}. In chapter \ref{cha:4}, we examine the performance of V-GIM compared to its non-variational counterpart GIM and provide an analysis of the learned representations. Chapter \ref{cha:5} presents a literature review of related work and compares it to V-GIM. Finally, in chapter \ref{cha:6} we conclude this thesis with a discussion on V-GIM's main findings and make suggestions for future work.\footnote{The structure of this thesis is based on Sindy Löwe's master's thesis in Artificial Intelligence from the University of Amsterdam.}. 



% nice text for background?

%Contrastive Predictive Coding (CPC), a successful self-supervised approach that compresses high 
%
%CPC can compress high dimensional data by maximally preserving the mutual information with the past and the future. We will discuss the mathematical foundations of CPC in more detail in the the following section.
%In recent years, CPC has been shown to be a successful self-supervised learning approach in a wide range of domains \citep{stackeEvaluationContrastivePredictive2020, dehaanContrastivePredictiveCoding2021, luSemiSupervisedHistologyClassification2019, bhatiSegmentalContrastivePredictive2021b, deldariTimeSeriesChange2021, henaffDataEfficientImageRecognition2020}. \cite{lowePuttingEndEndtoEnd2020a} contribute to this work splitting up a conventional neural network by depth and showing that the resulting modules can be trained greedily, each with their own instance of the InfoNCE loss, enabling modules to be trained in parallel, or sequentially. Moreover, Wang \citep{meihanwangSpeechRepresentationLearning2019} empirically demonstrates that GIM learns good speech representations for the speech dataset we use in our own experiments \ref{cha:experim_details_vgim}. However, the underlying representations obtained from optimising this InfoNCE objective have not yet been studied in great detail. 
%
%In this thesis we aim to gain better understanding in these representations by enforcing constraints to the latent space, resulting in a space that is easier to analyse and understand.



%----- bs:
%n unsupervised approach to learning representations for high dimensional sequential data, based on the mutual information maximisation introduced in Contrastive Predictive Coding.
%
%a self-supervised representation learning approach which learns representations for data. Through the introduction of stochasticity in the neural network and splitting up a conventional neural network architecture by depth, the outputs and 



%------
%interpretability is a concern, to understand neural network. This problem has only become more relevant in recent years, as in representation learning for instance, not only are the internal works of the ANNs unknown, but also its outputs. (eg autoencoder)
%
%To understand neural networks, we must first understand its representations before we can understand how it derives its answers. While multiple representation learning algorithms (InfoGAN) aim to find a solution to the first question, these approaches provide little information in the internal workings of the ANN.
%
%We attempt to go a step further,
%In this thesis we introduce V-GIM, by splitting up ANN into modules each obtaining interpretable representations, also providing interpretability inside the network.
%---------









%--------------
%
%alpha fold: protein problem (protein folding?)
%
%4 examples large impact:
%- ai can process data faster than humans. humans cannot read all books
%
%- time to sequence dna in half
%
%- complex simulation: gpt3: answer questions, write code, translate (all at same time)
%- deepmind's gato: atari games, stacking games, all same ANN
%----------------------

%While, previous interpretable representation learning has focused on learning representations for images, we work with sequential audio data, which is notoriously more complex as visualisation schemes no longe work, and small artificats in the audio may disturb observations.

%----------------------

%which aim to explain a model's decision. Methods range from adding constraints to internal workings, obtaining better interpretability, to developing additional techniques on top of the models to explain their predictions.

%----------------------


%What is a literature review?
%• A literature review generally also presents a short historical overview
%	• How did the field become the way it is?
%	• What is the effect of history?

%• However, the important thing is that it shows why and how your research is important
%	• So it needs to identify open questions and hot topics







%\begin{itemize}
%	\item Small section on biological neural networks leading into Sindy's gradient isolated learning
%	\item Neural networks lack explainability, important for use cases. E.g., our eventual use case: to see how a neural network learns speech, and if we can understand that, it may lead to insights in how humans do speech.
%	\item Our contributions
%\end{itemize}










% ---------------
%Doel:
%Onderwerp introduceren
%Lezer interesseren
%Relevantie aantonen van mijn studie
%Aantonen wat voor onzeroek in de scriptie volgt
%
%Delen:
%Aanleiding
%	- voorbeeld geven dat context beschrijft van onderzoek
%	- afbaken onderzoek
%	- huidige kennis:
%		- belangrijkste conclusies
%	- wat het nut is
%	- hoofdvragen + deelvragen
%	- lijswijzer
