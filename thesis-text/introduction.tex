\chapter{Introduction}
%https://www.scribbr.com/dissertation/introduction-structure/

\setcounter{page}{1} % Set the page counter to 3
\pagenumbering{arabic}


%%
AI extremely powerful, even potentially exceeding general artificial intelligence.

These approaches are typically trained using deep neural networks in an end-to-end fashion. While successful, their internal workings are usually not well understood due/Unfortunately, these systems still lack in explainability.

Multiple attempts have been made, (give few results in XAI)

% limitation 1) of XAI
However, strategies usually offer a visual solution that works well for visual images, but for sequential data becomes harder.

% limitation 2) of XAI
While understanding ANN's output gives part of the answer, still not quite true explainable, .. how does internal mechanism work?
 

In this thesis we introduce V-GIM, an unsupervised approach to learning representations for high dimensional sequential data, based on the mutual information maximisation introduced in Contrastive Predictive Coding.

a self-supervised representation learning approach which learns representations for data. Through the introduction of stochasticity in the neural network and splitting up a convential neural network architecture by depth, the outputs and 

In particular, in representation learning 

----
Our contributions our the following:
\begin{itemize}
	\item V-GIM enables better understanding of the learned representations, allowing to observe the learned data in each dimension, for both the final representations as internal activations. Thus providing better interpretability compared to other interpretable techniques such as InfoGAN, ...
	\item V-GIM offers computational benefits such as internal batch normalisation, outperforming its non-variational counterpart GIM. V-GIM's stochastic approach results in a more variability, potentially serving as data augmentation tool for downstream tasks with little labelled data.
	\item We provide insights in V-GIM's learned representations, making suggestions on which data in incorporated in which dimensions and which information may no longer be in the representations.
\end{itemize}




---

interpretability is a concern, to understand neural network. This problem has only become more relevant in recent years, as in representation learning for instance, not only are the internal works of the ANNs unknown, but also its outputs. (eg autoencoder)

To understand neural networks, we must first understand its representations before we can understand how it derives its answers. While multiple representation learning algorithms (infoGAN) aim to find a solution to the first question, these approaches provide little information in the internal workings of the ANN.

We attempt to go a step further,
In this thesis we introduce V-GIM, by splitting up ANN into modules each obtaining interpretable representations, also providing interpretability inside the network.

----


In recent years, CPC has been shown to be a successful self-supervised learning approach in a wide range of domains \citep{stackeEvaluationContrastivePredictive2020, dehaanContrastivePredictiveCoding2021, luSemiSupervisedHistologyClassification2019, bhatiSegmentalContrastivePredictive2021b, deldariTimeSeriesChange2021, henaffDataEfficientImageRecognition2020}. Löwe et al. contribute to this work by showing that modules can be trained greedily, each with their own instance of the InfoNCE loss, enabling modules to be trained in parallel, or sequentially. Moreover, Wang \citep{meihanwangSpeechRepresentationLearning2019} empirically demonstrates that GIM learns good speech representations for the speech dataset we use in our own experiments \ref{cha:experim_details_vgim}.

However, the underlying representations obtained from optimising this InfoNCE objective have not yet been studied in great detail. In this thesis we aim to gain better understanding in these representations by enforcing constraints to the latent space, resulting in a space that is easier to analyse and understand.



%While, previous interpretable representation learning has focused on learning representations for images, we work with sequential audio data, which is notoriously more complex as visualisation schemes no longe work, and small artificats in the audio may disturb observations.





%What is a literature review?
%• A literature review generally also presents a short historical overview
%	• How did the field become the way it is?
%	• What is the effect of history?

%• However, the important thing is that it shows why and how your research is important
%	• So it needs to identify open questions and hot topics







%\begin{itemize}
%	\item Small section on biological neural networks leading into Sindy's gradient isolated learning
%	\item Neural networks lack explainability, important for use cases. E.g., our eventual use case: to see how a neural network learns speech, and if we can understand that, it may lead to insights in how humans do speech.
%	\item Our contributions
%\end{itemize}










% ---------------
Doel:
Onderwerp introduceren
Lezer interesseren
Relevantie aantonen van mijn studie
Aantonen wat voor onzeroek in de scriptie volgt

Delen:
Aanleiding
	- voorbeeld geven dat context beschrijft van onderzoek
	- afbaken onderzoek
	- huidige kennis:
		- belangrijkste conclusies
	- wat het nut is
	- hoofdvragen + deelvragen
	- lijswijzer
