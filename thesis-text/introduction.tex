\chapter{Introduction}
%https://www.scribbr.com/dissertation/introduction-structure/

\setcounter{page}{1} % Set the page counter to 3
\pagenumbering{arabic}



Black-box models have shown incredible performance in recent years. Notable breakthroughs include AlphaGo defeating the world champion Go player in 2016 \citep{fuAlphaGoMonteCarlo2016}, and ChatGPT, which can assist with coding, reason through problems and generate well-written documents.

These models are typically trained using deep neural networks in an end-to-end fashion. Despite their impressive success, their lack of interpretability poses a significant challenge. The internal workings of these models are often not well understood, limiting their applicability in high-stakes decision-making scenarios, such as medical diagnosis, law or defence. A simple binary answer does not suffice; we need to ensure these models make decisions for the right reasons \citep{alammarExplainableAIGuide2021}. Additionally, a comprehensive understanding of these models is essential if humans wish to learn from them, for instance by applying strategies similar to those learned by AlphaGo.

While models that are interpretable by design do exist (e.g., logistic regression, linear classifiers and decision trees), they are limited by their simplicity and are not always able to model the complex problems in our world. Therefore, for certain problems, more complex models are required which result in reduced interpretability.

% post hoc methods
To gain better understanding of these less interpretable models, various post-hoc approaches have been explored. These approaches aim to find explanations for models that are not interpretable by design, for instance through techniques such as visual explanations, text explanations and feature attribution explanations \citep{barredoarrietaExplainableArtificialIntelligence2020a}.

Some feature attribution approaches include SHAP, LIME and gradient based methods, which compute a relevance or contribution score for each input feature with respect to the output prediction \citep{lundbergUnifiedApproachInterpreting2017, molnarInterpretableMachineLearning2022, simonyanDeepConvolutionalNetworks2014}. These approaches are frequently used for image classification via heath maps or salience, highlighting the important pixels that had the largest contribution to the decision.

While these approaches provide some insights into what the important key factors are that led to an individual prediction, they do not say much about the internal works of the model. Particularly, in artificial neural networks (ANN), it is known that specific neurons are sensitive for particular concepts. \citeauthor{zeilerVisualizingUnderstandingConvolutional2013} show that the initial layers in ConvNets trained for image classification, learn to recognise abstract concepts such as lines and edges, while deeper layers learn more semantically meaningful concepts such as the shape of a nose or ear, without being explicitly told to do so \citep{zeilerVisualizingUnderstandingConvolutional2013}.


Continuing in the line of post-hoc approaches, multiple explainability methods have been investigated which analyse the internal neurons, resulting in improved understand of the underlying mechanisms of these ANNs. Notable contributions aim to find the image that maximally activates a specific neuron \citep{simonyanDeepConvolutionalNetworks2014} (or, equivalently, a channel in ConvNets). A second approach aims to, highlight the regions of an image that a particular neuron is sensitive to \citep{zeilerVisualizingUnderstandingConvolutional2013}.

These post-hoc approaches provide some level of explainability to the predictions and the interpretability to the black-box models. However, many approaches focus on images, as their explanations are more easily interpretable. Conversely, when it comes to input features that are less visually interpretable, such as raw speech signals, utilising these explainability methods may be impractical for such models \citep{krugAnalyzingVisualizingDeep2021}.

In addition, \citeauthor{bauNetworkDissectionQuantifying2017} argue that the internal semantic concepts learned by these neurons are highly entangled throughout the network \cite{bauNetworkDissectionQuantifying2017}. This makes interpretation of neurons particularly difficult, as multiple neurons may work as a whole and together be sensitive for a semantic concept such as a nose \citep{molnarInterpretableMachineLearning2022}, meaning that there may not always be an individual neuron that is responsible for a specific concept. Additionally, these neurons may be even spread out over multiple layers.  Optimally, we require fully disentangled ANNs where each single neuron is sensitive for a unique feature. In addition, \citeauthor{bauNetworkDissectionQuantifying2017} found that many neurons could not be linked to a specific concept \citep{molnarInterpretableMachineLearning2022}.

For these reasons it is likely impossible to fully understand these black-box models. Instead, we pose the question of whether we can incorporate the interpretability requirements into the design of the network. Enforcing additional constraints, such as feature disentanglement, could potentially improve the interpretability of these ANNs, and improve the efficacy of these post-hoc explanations, while maintaining good performance. % TODO: zin moet beter

%This problem becomes even more important as we wish to use these learned features as alternative ``representations" for our data.


%Deep neural networks learn high-level features in the hidden layers. This is one of their greatest strengths and reduces the need for feature engineering."" - copied from book



%
%
%
%% ****MUST STILL BE CORRECTED****:
%		
%	%%% Towards representation learning
%		Neural networks learn abstract features internally, which is why a large part of the feature engineering is not necessary in deep learning, compared to traditional techniques such as SVM and Decision Trees % ref book.
%		
%		These learned features are useful on their own and could be applied for other classification tasks as well. For instance, a convolutional neural network tasked to predict tiger or elephant, may learn features such as edges, nose etc. that could potentially be used for classifying dogs vs cats as well. 
%		
%		Representation learning is concerned with finding lower dimensional features for high level data, independent on the task, which can be used for a wide range of purposes such as classification, regression or clustering.
%	
%	
%	In addition, these post-hoc explainability approaches are not directly compatible with the representations obtained from representation learning, a field which concerns itself of finding a mapping function between high dimensional data (eg: images or speech audio) to a lower dimension representation \cite{le-khacContrastiveRepresentationLearning2020}. This mapping function is commonly a neural network, and additionally, the representations are obtained used for a simple classifier for instance, but are usually non-interpretable to humans as well. 
%	
%		Explainability becomes an even larger concern in representation learning, which learn lower-dimensional representations for data, that can be used for instance for a classifier. As the output representations obtained from these models are non-interpretable, the above explainability approaches become useless. 
%% END OF TOODO



This thesis focuses on the representations obtained from the mutual information maximisation approach introduced by \cite{oordRepresentationLearningContrastive2019} in Contrastive Predictive Coding (CPC). CPC has demonstrated remarkable success in a wide range of domains, and is currently considered the state-of-the-art method for representation learning from sequential data without labels \citep{stackeEvaluationContrastivePredictive2020, dehaanContrastivePredictiveCoding2021, luSemiSupervisedHistologyClassification2019, bhatiSegmentalContrastivePredictive2021, deldariTimeSeriesChange2021, henaffDataEfficientImageRecognition2020}. However, the underlying representations obtained from optimising the InfoNCE objective in CPC have not been thoroughly studied.

To address these challenges, we introduce Variational Greedy InfoMax (V-GIM), a representation learning framework that learns interpretable representations by enforcing constraints on the latent space. V-GIM splits the architecture into modules, each trained greedily using a novel loss function that promotes interpretable representations. We introduce a Post-Hoc explainability approach via a decoder which utilises the constrained space to ensure meaningful explanations. This allows for the analysis of neurons at different depths in the architecture, enhancing interpretability for both the final and internal representations. In this thesis, we examine V-GIM's performance and analyse the interpretability of its learned representations, with a focus on raw speech waves.



Our contributions are the following:
\begin{itemize}
	\item V-GIM enables better understanding of the learned representations, enabling observation of the information in individual neurons, both in the final and internal representations. % This provides better interpretability compared to other interpretable techniques such as InfoGAN, ...
	\item V-GIM offers computational benefits, including internal batch normalisation, outperforming its non-variational counterpart GIM in certain cases. V-GIM's stochastic approach results in a more variability, potentially serving as data augmentation tool for downstream tasks with little labelled data.
	\item We use a decoder to gain insights into V-GIM's learned representations, making suggestions on which data is incorporated in which neuron and which information may no longer be present in the representations.
\end{itemize}




The structure of this thesis is as follows: in chapter \ref{cha:2}, we provide an overview of the necessary background information needed to fully understand our work, which is described in chapter \ref{cha:3}. In chapter \ref{cha:4}, we examine the performance of V-GIM compared to its non-variational counterpart GIM and provide an analysis of the learned representations. Chapter \ref{cha:5} presents a literature review of related work and compares it to V-GIM. Finally, in chapter \ref{cha:6} we conclude this thesis with a discussion on V-GIM's main findings and make suggestions for future work.\footnote{The structure of this thesis is based on Sindy Löwe's master's thesis in Artificial Intelligence from the University of Amsterdam.}. 



% nice text for background?

%Contrastive Predictive Coding (CPC), a successful self-supervised approach that compresses high 
%
%CPC can compress high dimensional data by maximally preserving the mutual information with the past and the future. We will discuss the mathematical foundations of CPC in more detail in the the following section.
%In recent years, CPC has been shown to be a successful self-supervised learning approach in a wide range of domains \citep{stackeEvaluationContrastivePredictive2020, dehaanContrastivePredictiveCoding2021, luSemiSupervisedHistologyClassification2019, bhatiSegmentalContrastivePredictive2021b, deldariTimeSeriesChange2021, henaffDataEfficientImageRecognition2020}. \cite{lowePuttingEndEndtoEnd2020a} contribute to this work splitting up a conventional neural network by depth and showing that the resulting modules can be trained greedily, each with their own instance of the InfoNCE loss, enabling modules to be trained in parallel, or sequentially. Moreover, Wang \citep{meihanwangSpeechRepresentationLearning2019} empirically demonstrates that GIM learns good speech representations for the speech dataset we use in our own experiments \ref{cha:experim_details_vgim}. However, the underlying representations obtained from optimising this InfoNCE objective have not yet been studied in great detail. 
%
%In this thesis we aim to gain better understanding in these representations by enforcing constraints to the latent space, resulting in a space that is easier to analyse and understand.



%----- bs:
%n unsupervised approach to learning representations for high dimensional sequential data, based on the mutual information maximisation introduced in Contrastive Predictive Coding.
%
%a self-supervised representation learning approach which learns representations for data. Through the introduction of stochasticity in the neural network and splitting up a conventional neural network architecture by depth, the outputs and 



%------
%interpretability is a concern, to understand neural network. This problem has only become more relevant in recent years, as in representation learning for instance, not only are the internal works of the ANNs unknown, but also its outputs. (eg autoencoder)
%
%To understand neural networks, we must first understand its representations before we can understand how it derives its answers. While multiple representation learning algorithms (InfoGAN) aim to find a solution to the first question, these approaches provide little information in the internal workings of the ANN.
%
%We attempt to go a step further,
%In this thesis we introduce V-GIM, by splitting up ANN into modules each obtaining interpretable representations, also providing interpretability inside the network.
%---------









%--------------
%
%alpha fold: protein problem (protein folding?)
%
%4 examples large impact:
%- ai can process data faster than humans. humans cannot read all books
%
%- time to sequence dna in half
%
%- complex simulation: gpt3: answer questions, write code, translate (all at same time)
%- deepmind's gato: atari games, stacking games, all same ANN
%----------------------

%While, previous interpretable representation learning has focused on learning representations for images, we work with sequential audio data, which is notoriously more complex as visualisation schemes no longe work, and small artificats in the audio may disturb observations.

%----------------------

%which aim to explain a model's decision. Methods range from adding constraints to internal workings, obtaining better interpretability, to developing additional techniques on top of the models to explain their predictions.

%----------------------


%What is a literature review?
%• A literature review generally also presents a short historical overview
%	• How did the field become the way it is?
%	• What is the effect of history?

%• However, the important thing is that it shows why and how your research is important
%	• So it needs to identify open questions and hot topics







%\begin{itemize}
%	\item Small section on biological neural networks leading into Sindy's gradient isolated learning
%	\item Neural networks lack explainability, important for use cases. E.g., our eventual use case: to see how a neural network learns speech, and if we can understand that, it may lead to insights in how humans do speech.
%	\item Our contributions
%\end{itemize}










% ---------------
%Doel:
%Onderwerp introduceren
%Lezer interesseren
%Relevantie aantonen van mijn studie
%Aantonen wat voor onzeroek in de scriptie volgt
%
%Delen:
%Aanleiding
%	- voorbeeld geven dat context beschrijft van onderzoek
%	- afbaken onderzoek
%	- huidige kennis:
%		- belangrijkste conclusies
%	- wat het nut is
%	- hoofdvragen + deelvragen
%	- lijswijzer
