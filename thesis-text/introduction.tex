\chapter{Introduction}
%https://www.scribbr.com/dissertation/introduction-structure/

\setcounter{page}{1} % Set the page counter to 3
\pagenumbering{arabic}


In recent years, CPC has been shown to be a successful self-supervised learning approach in a wide range of domains \cite{stackeEvaluationContrastivePredictive2020, dehaanContrastivePredictiveCoding2021, luSemiSupervisedHistologyClassification2019, bhatiSegmentalContrastivePredictive2021b, deldariTimeSeriesChange2021, henaffDataEfficientImageRecognition2020}. Löwe et al. contribute to this work by showing that modules can be trained greedily, each with their own instance of the InfoNCE loss, enabling modules to be trained in parallel, or sequentially. Moreover, Wang \cite{meihanwangSpeechRepresentationLearning2019} empirically demonstrates that GIM learns good speech representations for the speech dataset we use in our own experiments \ref{cha:experim_details_vgim}.

However, the underlying representations obtained from optimising this InfoNCE objective have not yet been studied in great detail. In this thesis we aim to gain better understanding in these representations by enforcing constraints to the latent space, resulting in a space that is easier to analyse and understand.






%What is a literature review?
%• A literature review generally also presents a short historical overview
%	• How did the field become the way it is?
%	• What is the effect of history?

%• However, the important thing is that it shows why and how your research is important
%	• So it needs to identify open questions and hot topics




%Referring with author and year
%• (Young 2010)
%• As shown by Young (2010) ...
%• (Stills & Young 1976)
%• (Crosby, Stills, Nash & Young 1969)
%• (Crosby et al. 1969) – subsequently
%• or always et al. if more than six authors
%• (Young 2010, 2012)
%• (Young 1975a, 1975b)
%• (Stills & Young 1976; Young 2010)













--------------


% TODO:
FEEDBACK BART

!!!
wat is + waarom het gedaan wordt.

should also explain in my work.

should be around 60 pages.

Discussion:
Sectie 4.5: (Bart wil aparte sectie)
- Hoe helpt deze techniek om het netwerk interpreteerbaarder te maken.
- compare techniques: whether they are better explainable.
- Zou na results moeten zijn. en kan zo inleiding zijn naar future work.
- bart zou verwachten dat cha 3 zeer groot is.

- UVA: wordt geschreven voor begeleiders, hun hebben meer achtergrond.
- Jury aan VUB andere verwachtingen, wil meer uitleg.

- mijn discussie sectie moet langer! en zeker in vertellen waarom het beter is voor visualities.

---
Defense:
- verduideluikende vragen
- critiek als gaten in argumentatie
- hun komen met suggestie die ze uit literatuur kennen, en moet bv mening over geven.
- "zou dat ook anders gekund hebben"
!!
---

%Order of importance: 
%Solution + evaluation, introduction + related work, 
%Fundamentals Future work


%\begin{itemize}
%	\item Small section on biological neural networks leading into Sindy's gradient isolated learning
%	\item Neural networks lack explainability, important for use cases. E.g., our eventual use case: to see how a neural network learns speech, and if we can understand that, it may lead to insights in how humans do speech.
%	\item Our contributions
%\end{itemize}

% Overview of thesis
\begin{itemize}
	\item Context: !E GIM for representation learning: generates representations that simplify classification tasks vs when done on raw data
	\item Problem: If wants to know for what tasks applicable... must know what is present in data
	\item Solution: Analysis of learned representations on speech data
	\item My contributions: 
	\begin{itemize}
		\item Decoder ANN for each layer of GIM: Shows what information is maintained through the layers
		\item Search correlations between kernels weights and signal features
		\item Extension on CPC via VAE
	\end{itemize}
\end{itemize}



% what a background is [generic]
%\begin{itemize}
%	\item Explains "what is" questions
%	\item State of the art neural networks take raw data as input and do pre-processing automatically
%	\item Explanation of Sindy's neural network
%	\item Techniques behind what I do
%\end{itemize}
%\hrule