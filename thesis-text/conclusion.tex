\chapter{Conclusion}

%can we improve interpretability of learning approaches through incorporating explainability requirements into the network, for CPC.

We introduced Variational Greedy InfoMax, a self-supervised representation learning approach which incorporates interpretability requirements into the design of the network. This approach enables the analysis of the internal representations through a decoder, ensuring that the decodings are not only meaningful but also that we can use them to obtain valuable insights into the underlying structure these representations.



In particular, we found that V-GIM learned internal representations which were sensitive to specific vowels, while consonant information was lacking. Furthermore, individual dimensions in the representations captured information on specific frequency bins or entire neighbourhoods of frequency bins. Meanwhile, certain dimensions did not appear to contain any information at all. This shows that the individual concepts that neurons are sensitive to, can be observed for both the output neurons but also the internal neurons, enabling us to better understand these networks internally, by observing the internal mechanisms these networks have learned.

In addition to the interpretability benefits, V-GIM's greedy learning approach combined with the constraints applied to the latent space of each module, results in an internal batch normalisation mechanism. This mechanism, leads to improved performance of the deeper modules, outperforming V-GIM's non-variational counterpart, Greedy InfoMax.

However, it is important to note that the representation variability obtained from V-GIM's stochastic representations does not appear to improve the performance of downstream tasks in scenarios where labelled data is scarce. Future work should investigate this in more detail, as these findings could potentially be attributed to the nature of the downstream task rather than the quality of the representations themselves.

%
%V-GIM enables easier analysis of the model through post-hoc explainability approaches, for instance, through a decoder.


% *** FOR INTRODUCTION OR ABSTRACT? ***
% V-GIM's latent space constraints ensure that a decoder trained on this space would generalise well to unseen data and predict new data points similar to the dataset, improving interpretability of reconstructions, allowing for meaningful interpolations between representations.

% vgims latent space constraint ensures that sampling a random point from a standard normal distribution is likely to correspond to a meaningful data point in the original space. As such, decoding the interpolation of two latent representations is more likely to result into a meaningful reconstruction, improving interpretability obtained form the decoder.


% (and ensure that the data corresponding to high neuron activations correspond to data points that are similar to the dataset)






%- results:
%	- found individual channels that were sensitive for specific to phonemes, a, u i ... meanwhile consonant information no longer seemed to be maintained in the representations.
%	- additionally certain dimensions were sensitive to frequencies
%	
%	-In addition, V-GIM's split up architecture resulted in other benefits such as an internal batch normalisation layer
%	
%	- meanwhile the representation variability in V-GIM's stochastic representations does not appear to improve performance of downstream tasks with few labelled data.
%	
%

%
%
%
%- short and general: dont discuss specific results, 
%- but broad statements that sum up the most important insights
%- do not introduce new data or interpretation

%reminder the reader:
%- why you took the approach
%- what you expected to find
%- how well the results matched expectations
%
%make recommendations
%
%emphasise contributions


