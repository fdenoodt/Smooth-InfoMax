import torch
import torch.nn as nn

from models import (
    loss_InfoNCE,
    autoregressor
)


class AutoregressorIndependentModule(nn.Module):
    def __init__(
        self, opt,
        nb_channels_cnn,
        nb_channels_regress,
        calc_accuracy=False,
        prediction_step=12
    ):
        super(AutoregressorIndependentModule, self).__init__()

        self.opt = opt
        self.calc_accuracy = calc_accuracy
        self.nb_channels_cnn = nb_channels_cnn
        self.nb_channels_regressor = nb_channels_regress

        self.autoregressor = autoregressor.Autoregressor(
            opt=opt, input_size=self.nb_channels_cnn, hidden_dim=self.nb_channels_regressor
        )

        self.loss = loss_InfoNCE.InfoNCE_Loss(
            opt,
            hidden_dim=self.nb_channels_regressor,
            enc_hidden=self.nb_channels_cnn,
            calc_accuracy=self.calc_accuracy,
            prediction_step=prediction_step
        )

    def get_latents(self, z):
        """
        Calculate the latent representation of the input (using both the encoder and the autoregressive model)
        :param x: batch with sampled audios (dimensions: B x C x L)
        :return: c - latent representation of the input (either the output of the autoregressor,
                if use_autoregressor=True, or the output of the encoder otherwise)
                z - latent representation generated by the encoder (or x if self.use_encoder=False)
                both of dimensions: B x L x C
        """
        z = z.permute(0, 2, 1)
        c = self.autoregressor(z)
        return c, z

    def forward(self, x):
        """
        combines all the operations necessary for calculating the loss and accuracy of the network given the input
        :param x: batch with sampled audios (dimensions: B x C x L)
        :return: total_loss - average loss over all samples, timesteps and prediction steps in the batch
                accuracies - average accuracies over all samples, timesteps and predictions steps in the batch
                c - latent representation of the input (either the output of the autoregressor,
                if use_autoregressor=True, or the output of the encoder otherwise)
        """

        # B x L x C = Batch size x #channels x length
        c, z = self.get_latents(x)  # B x L x C

        total_loss, accuracies = self.loss.get_loss(z, c)

        # for multi-GPU training
        total_loss = total_loss.unsqueeze(0)
        accuracies = accuracies.unsqueeze(0)

        nce_loss = total_loss
        kld_loss = torch.zeros_like(total_loss)

        return total_loss, accuracies, z, nce_loss, kld_loss
